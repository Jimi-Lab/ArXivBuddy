# Daily arXiv Papers
## Date: 2025-07-27
## Description: 用户自定义提示词：

我喜欢windows逆向

Zotero文献库分析：
### 主要研究方向
该用户的研究方向主要集中在**软件安全与人工智能的交叉领域**，核心关注点包括：
1. **漏洞检测与防御**：尤其关注深度学习（如Transformer、图神经网络）在静态/动态漏洞检测中的应用（如DeepDFA、DeepWukong、LineVul），以及大语言模型（LLM）在漏洞检测中的潜力（如GPT-4在漏洞检测中的表现）。
2. **大语言模型（LLM）安全**：研究LLM在代码分析、模糊测试、漏洞检测中的能力与局限性（如CodeBERT、LLM驱动的模糊测试），同时关注LLM的安全风险（如提示注入攻击、模型劫持、对抗样本生成）。
3. **AI驱动的攻击与防御**：探索AI模型（如深度学习、LLM）在网络安全中的双向应用，包括攻击（如对抗样本生成、模型后门）和防御（如联邦学习中的隐私保护、鲁棒性增强）。

---

### 感兴趣领域
1. **代码分析与程序理解**：
   - 预训练模型（如CodeBERT）在代码-自然语言双模态任务中的应用。
   - 二进制代码分析（如WebAssembly反编译、二进制代码相似性检测）。
   - 静态程序分析的并行化与中间表示（IR）优化。
2. **AI安全与对抗攻防**：
   - LLM的提示注入攻击（如HOUYI攻击框架）及防御（如设计模式、知识增强的代理框架CRAKEN）。
   - 模型后门攻击（如Imperio语言引导的后门）与防御（如联邦学习中的水印去除Sanitizer）。
   - 多模态系统的对抗攻击（如图像、文本、音频的联合攻击）。
3. **实际系统安全**：
   - 物联网（IoT）生态的LLM威胁检测框架。
   - 微服务架构中的污点分析（如MScan工具）。
   - 区块链欺诈检测的高效语言模型训练（如ZipZap框架）。

---

### 代表性主题
1. **漏洞检测的深度学习方法**：
   - 结合数据流分析与图神经网络（DeepDFA）提升检测效率。
   - 多基单元漏洞（MBU）对检测准确性的影响及改进框架。
   - LLM在漏洞检测中的零样本/小样本能力（如GPT-4超越传统方法）。
2. **LLM安全与代理通信**：
   - LLM代理通信协议（如MCP、A2A）的安全风险与防御。
   - 知识增强的LLM代理（如CRAKEN）在CTF挑战中的应用。
   - 针对LLM集成的红队测试经验（如100个生成式AI产品的对抗测试）。
3. **鲁棒性与隐私保护**：
   - 联邦学习中的高效隐私攻击（EPAFL）与防御（如无害水印）。
   - 个性化隐私保护掩码（P3-Mask）对抗非授权人脸识别。
   - 图神经网络（GNN）后门攻击与图缩减技术的鲁棒性分析。

---

### 常用方法
1. **深度学习与图模型**：
   - Transformer架构（如CodeBERT、基于注意力的模型）和GNN（如DeepWukong）用于代码表示与漏洞检测。
   - 对比学习（如BinCola）提升二进制代码相似性检测的跨域泛化能力。
2. **对抗攻防技术**：
   - 黑盒攻击（如HOUYI）与自适应防御（如STDLens对抗模型劫持）。
   - 集成学习（如FUSE）通过多样性增强检测鲁棒性。
3. **数据驱动优化**：
   - 基于LLM的漏洞样本生成（GVI）解决类别不平衡问题。
   - 轻量级模型压缩（如ZipZap的频率感知压缩）提升训练效率。
4. **静态与动态分析结合**：
   - 混合静态分析（如MScan的污点分析）与动态模糊测试（如LLM驱动的模糊测试）。

---

### 总结
该用户的研究具有鲜明的**跨学科特色**，聚焦于AI技术与软件安全的深度融合，核心目标是提升漏洞检测的自动化能力、增强AI模型的安全性，并探索实际系统中的攻防博弈。其工作兼顾理论创新（如新型鲁棒性指标、知识增强框架）与工程实践（如工具开发、红队测试），同时关注前沿挑战（如LLM代理通信安全、多模态攻击面）。未来可能进一步探索**AI驱动的自动化安全运维**和**可解释的AI安全分析**。

## Papers:
### 1. Learning to Locate: GNN-Powered Vulnerability Path Discovery in Open Source Code
#### Abstract:
Detecting security vulnerabilities in open-source software is a critical task that is highly regarded in the related research communities. Several approaches have been proposed in the literature for detecting vulnerable codes and identifying the classes of vulnerabilities. However, there is still room to work in explaining the root causes of detected vulnerabilities through locating vulnerable statements and the discovery of paths leading to the activation of the vulnerability. While frameworks like SliceLocator offer explanations by identifying vulnerable paths, they rely on rule-based sink identification that limits their generalization. In this paper, we introduce VulPathFinder, an explainable vulnerability path discovery framework that enhances SliceLocator's methodology by utilizing a novel Graph Neural Network (GNN) model for detecting sink statements, rather than relying on predefined rules. The proposed GNN captures semantic and syntactic dependencies to find potential sink points (PSPs), which are candidate statements where vulnerable paths end. After detecting PSPs, program slicing can be used to extract potentially vulnerable paths, which are then ranked by feeding them back into the target graph-based detector. Ultimately, the most probable path is returned, explaining the root cause of the detected vulnerability. We demonstrated the effectiveness of the proposed approach by performing evaluations on a benchmark of the buffer overflow CWEs from the SARD dataset, providing explanations for the corresponding detected vulnerabilities. The results show that VulPathFinder outperforms both original SliceLocator and GNNExplainer (as a general GNN explainability tool) in discovery of vulnerability paths to identified PSPs.
#### Summary:
这篇论文提出了VulPathFinder，一个可解释的漏洞路径发现框架，通过使用新颖的图神经网络（GNN）模型来检测sink语句，而不是依赖预定义的规则。该GNN捕获语义和句法依赖关系以找到潜在sink点（PSPs），然后使用程序切片提取潜在漏洞路径，并通过反馈到基于图的检测器中进行排名，最终返回最可能的路径以解释检测到的漏洞的根本原因。在SARD数据集的缓冲区溢出CWE基准测试中，VulPathFinder在发现漏洞路径方面优于原始SliceLocator和GNNExplainer。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17888

### 2. Information Security Based on LLM Approaches: A Review
#### Abstract:
Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.
#### Summary:
本文综述了大型语言模型（LLM）在信息安全领域的应用进展，包括恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别和密码算法优化等方面。论文分析了LLM基于神经网络和Transformer架构的技术基础及其在自然语言处理任务中的优势，指出LLM有助于提高安全系统的检测准确率和降低误报率。同时，论文也总结了当前应用成果，并指出在模型透明度、可解释性和场景适应性等方面仍面临挑战，需要进一步探索模型结构的优化和泛化能力的提升。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.18215

### 3. RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models
#### Abstract:
Resource Consumption Attacks (RCAs) have emerged as a significant threat to the deployment of Large Language Models (LLMs). With the integration of vision modalities, additional attack vectors exacerbate the risk of RCAs in large vision-language models (LVLMs). However, existing red-teaming studies have largely overlooked visual inputs as a potential attack surface, resulting in insufficient mitigation strategies against RCAs in LVLMs. To address this gap, we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on \textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first approach for exploiting visual modalities to trigger unbounded RCAs red-teaming. First, we present \textit{Vision Guided Optimization}, a fine-grained pixel-level optimization, to obtain \textit{Output Recall} adversarial perturbations, which can induce repeating output. Then, we inject the perturbations into visual inputs, triggering unbounded generations to achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective Parallel Losses} to generate universal attack templates and resolve optimization conflicts when intending to implement parallel attacks. Empirical results demonstrate that RECALLED increases service response latency by over 26 $\uparrow$, resulting in an additional 20\% increase in GPU utilization and memory consumption. Our study exposes security vulnerabilities in LVLMs and establishes a red-teaming framework that can facilitate future defense development against RCAs.
#### Summary:
论文《RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models》提出了一种针对大型视觉语言模型（LVLMs）的资源消耗攻击（RCAs）方法RECALLED。该方法通过视觉引导优化和像素级优化生成对抗性扰动，诱导模型重复输出，从而触发无限制的资源消耗。此外，作者还引入了多目标并行损失来生成通用攻击模板，解决并行攻击中的优化冲突。实验结果表明，RECALLED显著增加了服务响应延迟和GPU利用率，暴露了LVLMs的安全漏洞。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.18053

### 4. LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models
#### Abstract:
Language Models (LMs) typically adhere to a "pre-training and fine-tuning" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the "pre-training and fine-tuning" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.
#### Summary:
这篇论文提出了LoRA-Leak，一个针对LoRA微调语言模型的成员推理攻击（MIA）评估框架。研究发现，尽管LoRA微调仅调整少量参数，但利用预训练模型会导致更多信息泄露，使微调数据容易受到MIA攻击。论文评估了15种MIA方法（包括5种改进方法），并在三个先进语言模型和三个自然语言处理任务中验证了攻击效果（如AUC达0.775）。此外，还探讨了四种防御措施，发现仅dropout和排除特定层能有效降低风险。论文强调了预训练模型的存在加剧了LoRA微调模型的隐私风险。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.18302

### 5. Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering
#### Abstract:
The intellectual property of deep generative networks (GNets) can be protected using a cascaded hiding network (HNet) which embeds watermarks (or marks) into GNet outputs, known as box-free watermarking. Although both GNet and HNet are encapsulated in a black box (called operation network, or ONet), with only the generated and marked outputs from HNet being released to end users and deemed secure, in this paper, we reveal an overlooked vulnerability in such systems. Specifically, we show that the hidden GNet outputs can still be reliably estimated via query-based reverse engineering, leaking the generated and unmarked images, despite the attacker's limited knowledge of the system. Our first attempt is to reverse-engineer an inverse model for HNet under the stringent black-box condition, for which we propose to exploit the query process with specially curated input images. While effective, this method yields unsatisfactory image quality. To improve this, we subsequently propose an alternative method leveraging the equivalent additive property of box-free model watermarking and reverse-engineering a forward surrogate model of HNet, with better image quality preservation. Extensive experimental results on image processing and image generation tasks demonstrate that both attacks achieve impressive watermark removal success rates (100%) while also maintaining excellent image quality (reaching the highest PSNR of 34.69 dB), substantially outperforming existing attacks, highlighting the urgent need for robust defensive strategies to mitigate the identified vulnerability in box-free model watermarking.
#### Summary:
该论文揭示了深度学习生成网络（GNets）中盒免费水印技术的安全漏洞。通过基于查询的反向工程方法，攻击者可以在不了解系统内部细节的情况下，可靠地估计出隐藏的GNet输出，从而泄露未加水印的图像。论文提出了两种攻击方法：一种是反向工程HNet的逆模型，另一种是利用等效加性特性反向工程HNet的前向代理模型。实验结果表明，这两种方法都能成功移除水印（成功率100%）并保持较高的图像质量（最高PSNR达34.69 dB），突显了盒免费水印技术的脆弱性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.18034

### 6. NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN
#### Abstract:
The intellectual property of deep neural network (DNN) models can be protected with DNN watermarking, which embeds copyright watermarks into model parameters (white-box), model behavior (black-box), or model outputs (box-free), and the watermarks can be subsequently extracted to verify model ownership or detect model theft. Despite recent advances, these existing methods are inherently intrusive, as they either modify the model parameters or alter the structure. This natural intrusiveness raises concerns about watermarking-induced shifts in model behavior and the additional cost of fine-tuning, further exacerbated by the rapidly growing model size. As a result, model owners are often reluctant to adopt DNN watermarking in practice, which limits the development of practical Watermarking as a Service (WaaS) systems. To address this issue, we introduce Nonintrusive Watermarking as a Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in which we hypothesize that with the model untouched, an owner-defined watermark can still be extracted from model outputs. Building on this concept, we propose ShadowMark, a concrete implementation of NWaaS which addresses critical deployment challenges by establishing a robust and nonintrusive side channel in the protected model's black-box API, leveraging a key encoder and a watermark decoder. It is significantly distinctive from existing solutions by attaining the so-called absolute fidelity and being applicable to different DNN architectures, while being also robust against existing attacks, eliminating the fidelity-robustness trade-off. Extensive experiments on image-to-image, noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate the efficacy and practicality of ShadowMark for real-world deployment of nonintrusive DNN watermarking.
#### Summary:
这篇论文提出了一种名为NWaaS（Nonintrusive Watermarking as a Service）的非侵入式水印服务范式，旨在保护深度神经网络（DNN）模型的知识产权。与传统的侵入式水印方法不同，NWaaS通过在不修改模型参数或结构的情况下，从模型输出中提取所有者定义的水印，从而避免了模型行为的改变和额外的微调成本。论文还提出了ShadowMark作为NWaaS的具体实现，通过在受保护模型的黑盒API中建立健壮的非侵入式侧信道，解决了实际部署中的关键挑战。实验表明，ShadowMark在多种DNN架构中均表现出高效性和实用性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.18036

### 7. Scout: Leveraging Large Language Models for Rapid Digital Evidence Discovery
#### Abstract:
Recent technological advancements and the prevalence of technology in day to day activities have caused a major increase in the likelihood of the involvement of digital evidence in more and more legal investigations. Consumer-grade hardware is growing more powerful, with expanding memory and storage sizes and enhanced processor capabilities. Forensics investigators often have to sift through gigabytes of data during an ongoing investigation making the process tedious. Memory forensics, disk analysis all are well supported by state of the art tools that significantly lower the effort required to be put in by a forensic investigator by providing string searches, analyzing images file etc. During the course of the investigation a lot of false positives are identified that need to be lowered. This work presents Scout, a digital forensics framework that performs preliminary evidence processing and prioritizing using large language models. Scout deploys foundational language models to identify relevant artifacts from a large number of potential evidence files (disk images, captured network packets, memory dumps etc.) which would have taken longer to get identified. Scout employs text based large language models can easily process files with textual information. For the forensic analysis of multimedia files like audio, image, video, office documents etc. multimodal models are employed by Scout. Scout was able to identify and realize the evidence file that were of potential interest for the investigator.
#### Summary:
这篇论文介绍了Scout，一个利用大语言模型（LLM）进行数字证据发现的法医框架。Scout通过部署基础语言模型和多模态模型，从大量潜在证据文件（如磁盘映像、网络数据包、内存转储等）中快速识别相关证据，减少法医调查中的误报和工作量。该框架能够处理文本信息和多媒体文件，帮助调查人员优先处理关键证据。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.18478

### 8. Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment
#### Abstract:
With rapid advancement and increasing accessibility of LLMs, fine-tuning aligned models has become a critical step for adapting them to real-world applications, which makes the safety of this fine-tuning process more important than ever. However, recent studies have highlighted a critical challenge: even when fine-tuning with seemingly benign downstream datasets, the safety of aligned LLMs can be compromised, making them more susceptible to malicious instructions. In this paper, we show that fine-tuning datasets often contain samples with safety-degrading features that are not easily identifiable on the surface. These samples can significantly degrade the safety alignment of LLMs during fine-tuning. To address this issue, we propose LARF, a \textbf{L}ayer-\textbf{A}ware \textbf{R}epresentation \textbf{F}iltering method. This method identifies safety-sensitive layers within the LLM and leverages their representations to detect which data samples in the post-training dataset contain safety-degrading features. Experimental results demonstrate that LARF can effectively identify benign data with safety-degrading features. After removing such data, the safety alignment degradation caused by fine-tuning is mitigated. Please see our code at \href{this https URL}{this https URL}.
#### Summary:
该论文提出了LARF（Layer-Aware Representation Filtering）方法，用于识别和过滤微调数据中可能损害大语言模型（LLM）安全对齐的样本。通过分析LLM中安全敏感的层，LARF能够检测并移除这些具有安全降级特征的样本，从而在微调过程中保持模型的安全性。实验结果表明，LARF能有效缓解因微调导致的安全对齐退化问题。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.18631

### 9. PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python
#### Abstract:
Python software development heavily relies on third-party packages. Direct and transitive dependencies create a labyrinth of software supply chains. While it is convenient to reuse code, vulnerabilities within these dependency chains can propagate through dependencies, potentially affecting down-stream packages and applications. PyPI, the official Python package repository, hosts many packages and lacks a comprehensive analysis of the prevalence of vulnerable dependencies. This paper introduces PyPitfall, a quantitative analysis of vulnerable dependencies across the PyPI ecosystem. We analyzed the dependency structures of 378,573 PyPI packages and identified 4,655 packages that explicitly require at least one known-vulnerable version and 141,044 packages that permit vulnerable versions within specified ranges. By characterizing the ecosystem-wide dependency landscape and the security impact of transitive dependencies, we aim to raise awareness of Python software supply chain security.
#### Summary:
这篇论文《PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python》主要研究了Python软件供应链中的依赖混乱和漏洞传播问题。作者分析了PyPI生态系统中378,573个包的依赖结构，发现4,655个包明确要求至少一个已知易受攻击的版本，141,044个包允许在指定范围内使用易受攻击的版本。研究旨在提高对Python软件供应链安全的认识。
#### Relevance Score: 5.0
#### PDF URL: https://arxiv.org/pdf/2507.18075

### 10. TimelyHLS: LLM-Based Timing-Aware and Architecture-Specific FPGA HLS Optimization
#### Abstract:
Achieving timing closure and design-specific optimizations in FPGA-targeted High-Level Synthesis (HLS) remains a significant challenge due to the complex interaction between architectural constraints, resource utilization, and the absence of automated support for platform-specific pragmas. In this work, we propose TimelyHLS, a novel framework integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automatically generate and iteratively refine HLS code optimized for FPGA-specific timing and performance requirements. TimelyHLS is driven by a structured architectural knowledge base containing FPGA-specific features, synthesis directives, and pragma templates. Given a kernel, TimelyHLS generates HLS code annotated with both timing-critical and design-specific pragmas. The synthesized RTL is then evaluated using commercial toolchains, and simulation correctness is verified against reference outputs via custom testbenches. TimelyHLS iteratively incorporates synthesis logs and performance reports into the LLM engine for refinement in the presence of functional discrepancies. Experimental results across 10 FPGA architectures and diverse benchmarks show that TimelyHLS reduces the need for manual tuning by up to 70%, while achieving up to 4x latency speedup (e.g., 3.85x for Matrix Multiplication, 3.7x for Bitonic Sort) and over 50% area savings in certain cases (e.g., 57% FF reduction in Viterbi). TimelyHLS consistently achieves timing closure and functional correctness across platforms, highlighting the effectiveness of LLM-driven, architecture-aware synthesis in automating FPGA design.
#### Summary:
这篇论文提出了TimelyHLS，一个结合大型语言模型（LLMs）和检索增强生成（RAG）的框架，用于自动生成和迭代优化针对FPGA特定时序和性能需求的高层次综合（HLS）代码。TimelyHLS通过结构化知识库驱动，生成带有时序关键和设计特定pragma注释的HLS代码，并通过商业工具链评估和验证功能正确性。实验结果表明，TimelyHLS显著减少了手动调优的需求，并在多种FPGA架构上实现了显著的性能提升和资源节省。
#### Relevance Score: 4.0
#### PDF URL: https://arxiv.org/pdf/2507.17962

### 11. An Improved ChaCha Algorithm Based on Quantum Random Number
#### Abstract:
Due to the merits of high efficiency and strong security against timing and side-channel attacks, ChaCha has been widely applied in real-time communication and data streaming scenarios. However, with the rapid development of AI-assisted cryptanalysis and quantum computing technologies, there are serious challenges to the secure implementation of ChaCha cipher. To further strengthen the security of ChaCha cipher, we propose an improved variant based on quantum random numbers, i.e., Quantum Random Number Enhanced ChaCha (QRE-ChaCha). Specifically, the design XORs the initial constants with quantum random numbers and periodically injects quantum random numbers into selected state words during odd rounds to enhance diffusion. Compared with the original ChaCha, the present variant shows stronger resistance to differential attacks and generates a keystream with statistical randomness, thereby offering increased robustness against both classical and quantum attacks. To evaluate the security and performance of the present ChaCha, our analysis proceeds in three main parts. Firstly, we analyze its theoretical security in terms of quantum randomness and attack testing, and conduct differential cryptanalysis with an automated search method based on the Boolean satisfiability problem (SAT). Secondly, we subject the keystream generated by the cipher to randomness tests using the NIST statistical test suite and the GM/T 0005-2021 randomness testing standard. Finally, we assess its encryption and decryption performance by measuring its encryption speed on files of various sizes. According to the results, the present ChaCha is significantly improved to resist differential attacks while maintaining the high efficiency of the original ChaCha cipher, and its keystream successfully passes statistical randomness tests using the NIST and GM/T 0005-2021 standards, meeting cryptographic application requirements.
#### Summary:
该论文提出了一种基于量子随机数的改进版ChaCha算法（QRE-ChaCha），通过在初始常数中引入量子随机数并在奇数轮周期性地将量子随机数注入状态字，增强了算法的扩散性和抗差分攻击能力。论文从理论安全性分析（包括量子随机性和SAT自动搜索的差分密码分析）、NIST和GM/T 0005-2021标准的随机性测试，以及不同文件大小的加密性能测试三个方面评估了该算法的安全性和效率。结果表明，QRE-ChaCha在保持原算法高效性的同时，显著提升了抗攻击能力，并满足密码学应用的随机性要求。
#### Relevance Score: 4.0
#### PDF URL: https://arxiv.org/pdf/2507.18157

### 12. Auto-SGCR: Automated Generation of Smart Grid Cyber Range Using IEC 61850 Standard Models
#### Abstract:
Digitalization of power grids have made them increasingly susceptible to cyber-attacks in the past decade. Iterative cybersecurity testing is indispensable to counter emerging attack vectors and to ensure dependability of critical infrastructure. Furthermore, these can be used to evaluate cybersecurity configuration, effectiveness of the cybersecurity measures against various attack vectors, as well as to train smart grid cybersecurity experts defending the system. Enabling extensive experiments narrows the gap between academic research and production environment. A high-fidelity cyber range is vital as it is often infeasible to conduct such experiments and training using production environment. However, the design and implementation of cyber range requires extensive domain knowledge of physical and cyber aspect of the infrastructure. Furthermore, costs incurred for setup and maintenance of cyber range are significant. Moreover, most existing smart grid cyber ranges are designed as a one-off, proprietary system, and are limited in terms of configurability, accessibility, portability, and reproducibility. To address these challenges, an automated Smart grid Cyber Range generation framework is presented in this paper. Initially a human-/machine-friendly, XML-based modeling language called Smart Grid Modeling Language was defined, which incorporates IEC 61850 System Configuration Language files. Subsequently, a toolchain to parse SG-ML model files and automatically instantiate a functional smart grid cyber range was developed. The developed SG-ML models can be easily shared and/or modified to reproduce or customize for any cyber range. The application of Auto-SGCR is demonstrated through case studies with large-scale substation models. The toolchain along with example SG-ML models have been open-sourced.
#### Summary:
这篇论文提出了一个名为Auto-SGCR的自动化框架，用于生成基于IEC 61850标准模型的高保真智能电网网络靶场（cyber range）。该框架通过定义一种名为智能电网建模语言（SG-ML）的XML格式，结合IEC 61850系统配置语言文件，实现智能电网网络靶场的自动实例化。论文还开发了一个工具链来解析SG-ML模型文件，并支持大规模变电站模型的案例研究。该框架旨在解决现有智能电网网络靶场在可配置性、可访问性、可移植性和可重复性方面的局限性。
#### Relevance Score: 4.0
#### PDF URL: https://arxiv.org/pdf/2507.18249

### 13. Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment
#### Abstract:
The deployment of large-scale software-based 5G core functions presents significant challenges due to their reliance on optimized and intelligent resource provisioning for their services. Many studies have focused on analyzing the impact of resource allocation for complex deployments using mathematical models, queue theories, or even Artificial Intelligence (AI). This paper elucidates the effects of chaotic workloads, generated by Distributed Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment registration performance. Our findings highlight the necessity of diverse resource profiles to ensure Service-Level Agreement (SLA) compliance in large-scale 5G core deployments. Additionally, our analysis of packet capture approaches demonstrates the potential of kernel-based monitoring for scalable security threat defense. Finally, our empirical evaluation provides insights into the effective deployment of 5G NFs in complex scenarios.
#### Summary:
这篇论文探讨了大规模软件化5G核心网部署中的性能评估与威胁缓解问题，重点分析了分布式拒绝服务（DDoS）攻击产生的混沌工作负载对不同网络功能（NFs）在用户设备注册性能上的影响。研究强调了多样化资源分配对满足服务级别协议（SLA）的重要性，并验证了基于内核的监控在可扩展安全威胁防御中的潜力。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17850

### 14. Formal Verification of the Safegcd Implementation
#### Abstract:
The modular inverse is an essential piece of computation required for elliptic curve operations used for digital signatures in Bitcoin and other applications. A novel approach to the extended Euclidean algorithm has been developed by Bernstein and Yang within the last few years and incorporated into the libsecp256k1 cryptographic library used by Bitcoin. However, novel algorithms introduce new risks of errors. To address this we have completed a computer verified proof of the correctness of (one of) libsecp256k1's modular inverse implementations with the Coq proof assistant using the Verifiable C's implementation of separation logic.
#### Summary:
这篇论文主要研究了比特币等应用中用于椭圆曲线数字签名的模逆计算的安全实现。作者针对Bernstein和Yang提出的新型扩展欧几里得算法（已集成到比特币使用的libsecp256k1密码库中），使用Coq证明助手和Verifiable C的分离逻辑实现，完成了对该库模逆实现正确性的计算机验证证明。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17956

### 15. MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection
#### Abstract:
Phishing emails continue to pose a significant threat to cybersecurity by exploiting human vulnerabilities through deceptive content and malicious payloads. While Machine Learning (ML) models are effective at detecting phishing threats, their performance largely relies on the quality and diversity of the training data. This paper presents MeAJOR (Merged email Assets from Joint Open-source Repositories) Corpus, a novel, multi-source phishing email dataset designed to overcome critical limitations in existing resources. It integrates 135894 samples representing a broad number of phishing tactics and legitimate emails, with a wide spectrum of engineered features. We evaluated the dataset's utility for phishing detection research through systematic experiments with four classification models (RF, XGB, MLP, and CNN) across multiple feature configurations. Results highlight the dataset's effectiveness, achieving 98.34% F1 with XGB. By integrating broad features from multiple categories, our dataset provides a reusable and consistent resource, while addressing common challenges like class imbalance, generalisability and reproducibility.
#### Summary:
这篇论文介绍了MeAJOR Corpus，一个多源的钓鱼邮件检测数据集，旨在解决现有资源中的关键限制。该数据集整合了135,894个样本，涵盖了广泛的钓鱼策略和合法邮件，并包含多种工程特征。通过四种分类模型（RF、XGB、MLP和CNN）的系统实验，验证了数据集的有效性，XGB模型达到了98.34%的F1分数。该数据集解决了类别不平衡、泛化性和可重复性等常见挑战。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17978

### 16. Conformidade com os Requisitos Legais de Privacidade de Dados: Um Estudo sobre Técnicas de Anonimização
#### Abstract:
The protection of personal data has become a central topic in software development, especially with the implementation of the General Data Protection Law (LGPD) in Brazil and the General Data Protection Regulation (GDPR) in the European Union. With the enforcement of these laws, certain software quality criteria have become mandatory, such as data anonymization, which is one of the main aspects addressed by these regulations. The aim of this article is to analyze data anonymization techniques and assess their effectiveness in ensuring compliance with legal requirements and the utility of the data for its intended purpose. Techniques such as aggregation, generalization, perturbation, and k-anonymity were investigated and applied to datasets containing personal and sensitive data. The analysis revealed significant variations in the effectiveness of each method, highlighting the need to balance privacy and data utility.
#### Summary:
这篇论文研究了数据匿名化技术在满足巴西《通用数据保护法》（LGPD）和欧盟《通用数据保护条例》（GDPR）等法律要求方面的有效性。作者分析了多种匿名化技术（如聚合、泛化、扰动和k-匿名）在保护个人和敏感数据时的表现，并探讨了隐私保护与数据实用性之间的平衡。
#### Relevance Score: 2.0
#### PDF URL: https://arxiv.org/pdf/2507.18360

