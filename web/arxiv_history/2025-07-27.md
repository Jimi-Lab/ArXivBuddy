# Daily arXiv Papers
## Date: 2025-07-27
## Description: 用户自定义提示词：

我喜欢逆向

Zotero文献库分析：
### 主要研究方向
该用户的研究方向主要集中在**软件安全与人工智能的交叉领域**，核心关注点包括：
1. **漏洞检测与防御**：尤其关注深度学习（如Transformer、图神经网络）在静态/动态漏洞检测中的应用（如DeepDFA、DeepWukong、LineVul），以及大语言模型（LLM）在漏洞检测中的潜力（如GPT-4在漏洞检测中的表现）。
2. **大语言模型（LLM）安全**：研究LLM在代码分析、模糊测试、漏洞检测中的能力与局限性（如CodeBERT、LLM驱动的模糊测试），同时关注LLM的安全风险（如提示注入攻击、模型劫持、对抗样本生成）。
3. **AI驱动的攻击与防御**：探索AI模型（如深度学习、LLM）在网络安全中的双向应用，包括攻击（如对抗样本生成、模型后门）和防御（如联邦学习中的隐私保护、鲁棒性增强）。

---

### 感兴趣领域
1. **代码分析与程序理解**：
   - 预训练模型（如CodeBERT）在代码-自然语言双模态任务中的应用。
   - 二进制代码分析（如WebAssembly反编译、二进制代码相似性检测）。
   - 静态程序分析的并行化与中间表示（IR）优化。
2. **AI安全与对抗攻防**：
   - LLM的提示注入攻击（如HOUYI攻击框架）及防御（如设计模式、知识增强的代理框架CRAKEN）。
   - 模型后门攻击（如Imperio语言引导的后门）与防御（如联邦学习中的水印去除Sanitizer）。
   - 多模态系统的对抗攻击（如图像、文本、音频的联合攻击）。
3. **实际系统安全**：
   - 物联网（IoT）生态的LLM威胁检测框架。
   - 微服务架构中的污点分析（如MScan工具）。
   - 区块链欺诈检测的高效语言模型训练（如ZipZap框架）。

---

### 代表性主题
1. **漏洞检测的深度学习方法**：
   - 结合数据流分析与图神经网络（DeepDFA）提升检测效率。
   - 多基单元漏洞（MBU）对检测准确性的影响及改进框架。
   - LLM在漏洞检测中的零样本/小样本能力（如GPT-4超越传统方法）。
2. **LLM安全与代理通信**：
   - LLM代理通信协议（如MCP、A2A）的安全风险与防御。
   - 知识增强的LLM代理（如CRAKEN）在CTF挑战中的应用。
   - 针对LLM集成的红队测试经验（如100个生成式AI产品的对抗测试）。
3. **鲁棒性与隐私保护**：
   - 联邦学习中的高效隐私攻击（EPAFL）与防御（如无害水印）。
   - 个性化隐私保护掩码（P3-Mask）对抗非授权人脸识别。
   - 图神经网络（GNN）后门攻击与图缩减技术的鲁棒性分析。

---

### 常用方法
1. **深度学习与图模型**：
   - Transformer架构（如CodeBERT、基于注意力的模型）和GNN（如DeepWukong）用于代码表示与漏洞检测。
   - 对比学习（如BinCola）提升二进制代码相似性检测的跨域泛化能力。
2. **对抗攻防技术**：
   - 黑盒攻击（如HOUYI）与自适应防御（如STDLens对抗模型劫持）。
   - 集成学习（如FUSE）通过多样性增强检测鲁棒性。
3. **数据驱动优化**：
   - 基于LLM的漏洞样本生成（GVI）解决类别不平衡问题。
   - 轻量级模型压缩（如ZipZap的频率感知压缩）提升训练效率。
4. **静态与动态分析结合**：
   - 混合静态分析（如MScan的污点分析）与动态模糊测试（如LLM驱动的模糊测试）。

---

### 总结
该用户的研究具有鲜明的**跨学科特色**，聚焦于AI技术与软件安全的深度融合，核心目标是提升漏洞检测的自动化能力、增强AI模型的安全性，并探索实际系统中的攻防博弈。其工作兼顾理论创新（如新型鲁棒性指标、知识增强框架）与工程实践（如工具开发、红队测试），同时关注前沿挑战（如LLM代理通信安全、多模态攻击面）。未来可能进一步探索**AI驱动的自动化安全运维**和**可解释的AI安全分析**。

## Papers:
### 1. Learning to Locate: GNN-Powered Vulnerability Path Discovery in Open Source Code
#### Abstract:
Detecting security vulnerabilities in open-source software is a critical task that is highly regarded in the related research communities. Several approaches have been proposed in the literature for detecting vulnerable codes and identifying the classes of vulnerabilities. However, there is still room to work in explaining the root causes of detected vulnerabilities through locating vulnerable statements and the discovery of paths leading to the activation of the vulnerability. While frameworks like SliceLocator offer explanations by identifying vulnerable paths, they rely on rule-based sink identification that limits their generalization. In this paper, we introduce VulPathFinder, an explainable vulnerability path discovery framework that enhances SliceLocator's methodology by utilizing a novel Graph Neural Network (GNN) model for detecting sink statements, rather than relying on predefined rules. The proposed GNN captures semantic and syntactic dependencies to find potential sink points (PSPs), which are candidate statements where vulnerable paths end. After detecting PSPs, program slicing can be used to extract potentially vulnerable paths, which are then ranked by feeding them back into the target graph-based detector. Ultimately, the most probable path is returned, explaining the root cause of the detected vulnerability. We demonstrated the effectiveness of the proposed approach by performing evaluations on a benchmark of the buffer overflow CWEs from the SARD dataset, providing explanations for the corresponding detected vulnerabilities. The results show that VulPathFinder outperforms both original SliceLocator and GNNExplainer (as a general GNN explainability tool) in discovery of vulnerability paths to identified PSPs.
#### Summary:
论文提出 VulPathFinder 框架，旨在为开源代码中的漏洞提供可解释的根因定位。核心思路是用 Graph Neural Network（GNN）自动识别潜在的 sink 语句（潜在漏洞终点），替代传统规则方法；随后通过程序切片提取从源到 sink 的完整路径，并用目标漏洞检测器对路径排序，最终返回最可能的漏洞路径，实现“解释了漏洞是如何被触发的”。作者在 SARD 的缓冲区溢出 CWE 数据集上实验，结果显示 VulPathFinder 在发现漏洞路径的准确性上优于 SliceLocator 和通用 GNN 解释工具 GNNExplainer。
#### Relevance Score: 9.0
#### PDF URL: https://arxiv.org/pdf/2507.17888

### 2. RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models
#### Abstract:
Resource Consumption Attacks (RCAs) have emerged as a significant threat to the deployment of Large Language Models (LLMs). With the integration of vision modalities, additional attack vectors exacerbate the risk of RCAs in large vision-language models (LVLMs). However, existing red-teaming studies have largely overlooked visual inputs as a potential attack surface, resulting in insufficient mitigation strategies against RCAs in LVLMs. To address this gap, we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on \textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first approach for exploiting visual modalities to trigger unbounded RCAs red-teaming. First, we present \textit{Vision Guided Optimization}, a fine-grained pixel-level optimization, to obtain \textit{Output Recall} adversarial perturbations, which can induce repeating output. Then, we inject the perturbations into visual inputs, triggering unbounded generations to achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective Parallel Losses} to generate universal attack templates and resolve optimization conflicts when intending to implement parallel attacks. Empirical results demonstrate that RECALLED increases service response latency by over 26 $\uparrow$, resulting in an additional 20\% increase in GPU utilization and memory consumption. Our study exposes security vulnerabilities in LVLMs and establishes a red-teaming framework that can facilitate future defense development against RCAs.
#### Summary:
论文首次提出针对大视觉-语言模型（LVLM）的无限资源耗尽攻击 RECALLED。作者发现现有红队测试忽视了视觉输入这一攻击面，于是设计了“视觉引导优化”算法，在像素级生成能让模型重复输出的对抗扰动，并通过“多目标并行损失”解决并行攻击时的优化冲突。实验表明，该方法能把服务响应延迟提高 26 倍以上，并额外增加 20% GPU 利用率与内存消耗，从而揭示 LVLM 在资源消耗攻击下的新脆弱点。
#### Relevance Score: 9.0
#### PDF URL: https://arxiv.org/pdf/2507.18053

### 3. Information Security Based on LLM Approaches: A Review
#### Abstract:
Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.
#### Summary:
这篇综述论文系统梳理了基于大语言模型（LLM）的信息安全研究进展，重点覆盖恶意行为预测、网络威胁分析、系统漏洞检测、恶意代码识别及密码算法优化五大场景；分析了Transformer架构在提升检测精度、降低误报率方面的优势；同时指出模型可解释性、场景适应性、结构优化与泛化能力仍是主要挑战。
#### Relevance Score: 9.0
#### PDF URL: https://arxiv.org/pdf/2507.18215

### 4. LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models
#### Abstract:
Language Models (LMs) typically adhere to a "pre-training and fine-tuning" paradigm, where a universal pre-trained model can be fine-tuned to cater to various specialized domains. Low-Rank Adaptation (LoRA) has gained the most widespread use in LM fine-tuning due to its lightweight computational cost and remarkable performance. Because the proportion of parameters tuned by LoRA is relatively small, there might be a misleading impression that the LoRA fine-tuning data is invulnerable to Membership Inference Attacks (MIAs). However, we identify that utilizing the pre-trained model can induce more information leakage, which is neglected by existing MIAs. Therefore, we introduce LoRA-Leak, a holistic evaluation framework for MIAs against the fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership inference attacks, including ten existing MIAs, and five improved MIAs that leverage the pre-trained model as a reference. In experiments, we apply LoRA-Leak to three advanced LMs across three popular natural language processing tasks, demonstrating that LoRA-based fine-tuned LMs are still vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings). We also applied LoRA-Leak to different fine-tuning settings to understand the resulting privacy risks. We further explore four defenses and find that only dropout and excluding specific LM layers during fine-tuning effectively mitigate MIA risks while maintaining utility. We highlight that under the "pre-training and fine-tuning" paradigm, the existence of the pre-trained model makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can provide guidance on data privacy protection for specialized LM providers.
#### Summary:
本文提出并系统评估了LoRA-Leak——一套针对LoRA微调后语言模型的会员推断攻击（MIA）框架。作者指出，虽然LoRA仅更新极少参数，看似隐私风险低，但预训练模型本身会泄露额外信息。LoRA-Leak整合了15种MIA（含5种利用预训练模型作参照的新攻击），在三大任务、三大模型上实验显示AUC可达0.775。论文还测试了四种防御手段：仅dropout和屏蔽特定层能在保持模型效用的同时降低攻击成功率。结论强调在“预训练-微调”范式下，预训练模型的存在使LoRA微调模型对MIA更为敏感，需加强数据隐私保护。
#### Relevance Score: 9.0
#### PDF URL: https://arxiv.org/pdf/2507.18302

### 5. Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment
#### Abstract:
With rapid advancement and increasing accessibility of LLMs, fine-tuning aligned models has become a critical step for adapting them to real-world applications, which makes the safety of this fine-tuning process more important than ever. However, recent studies have highlighted a critical challenge: even when fine-tuning with seemingly benign downstream datasets, the safety of aligned LLMs can be compromised, making them more susceptible to malicious instructions. In this paper, we show that fine-tuning datasets often contain samples with safety-degrading features that are not easily identifiable on the surface. These samples can significantly degrade the safety alignment of LLMs during fine-tuning. To address this issue, we propose LARF, a \textbf{L}ayer-\textbf{A}ware \textbf{R}epresentation \textbf{F}iltering method. This method identifies safety-sensitive layers within the LLM and leverages their representations to detect which data samples in the post-training dataset contain safety-degrading features. Experimental results demonstrate that LARF can effectively identify benign data with safety-degrading features. After removing such data, the safety alignment degradation caused by fine-tuning is mitigated. Please see our code at \href{this https URL}{this https URL}.
#### Summary:
本文指出，即使使用表面无害的数据对已经安全对齐的LLM进行微调，也可能因数据中隐含的“安全退化特征”而破坏模型的安全对齐，使模型更易受到恶意指令影响。为此，作者提出LARF（Layer-Aware Representation Filtering）方法：先定位LLM中对安全最敏感的若干层，再利用这些层的内部表示来检测并剔除训练集中具有安全退化特征的样本。实验证明，LARF能有效识别并过滤这些“看似无害、实则有害”的数据，显著缓解微调带来的安全对齐下降问题。代码已开源。
#### Relevance Score: 9.0
#### PDF URL: https://arxiv.org/pdf/2507.18631

### 6. PyPitfall: Dependency Chaos and Software Supply Chain Vulnerabilities in Python
#### Abstract:
Python software development heavily relies on third-party packages. Direct and transitive dependencies create a labyrinth of software supply chains. While it is convenient to reuse code, vulnerabilities within these dependency chains can propagate through dependencies, potentially affecting down-stream packages and applications. PyPI, the official Python package repository, hosts many packages and lacks a comprehensive analysis of the prevalence of vulnerable dependencies. This paper introduces PyPitfall, a quantitative analysis of vulnerable dependencies across the PyPI ecosystem. We analyzed the dependency structures of 378,573 PyPI packages and identified 4,655 packages that explicitly require at least one known-vulnerable version and 141,044 packages that permit vulnerable versions within specified ranges. By characterizing the ecosystem-wide dependency landscape and the security impact of transitive dependencies, we aim to raise awareness of Python software supply chain security.
#### Summary:
PyPitfall 系统性地扫描了 PyPI 上 378,573 个包，量化刻画了依赖链中已知漏洞的传播规模：4,655 个包硬性依赖存在 CVE 的特定版本，141,044 个包在版本约束里允许引入漏洞版本。作者通过构建依赖图并分析传递性影响，揭示了 Python 生态软件供应链的整体安全风险结构。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.18075

### 7. Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering
#### Abstract:
The intellectual property of deep generative networks (GNets) can be protected using a cascaded hiding network (HNet) which embeds watermarks (or marks) into GNet outputs, known as box-free watermarking. Although both GNet and HNet are encapsulated in a black box (called operation network, or ONet), with only the generated and marked outputs from HNet being released to end users and deemed secure, in this paper, we reveal an overlooked vulnerability in such systems. Specifically, we show that the hidden GNet outputs can still be reliably estimated via query-based reverse engineering, leaking the generated and unmarked images, despite the attacker's limited knowledge of the system. Our first attempt is to reverse-engineer an inverse model for HNet under the stringent black-box condition, for which we propose to exploit the query process with specially curated input images. While effective, this method yields unsatisfactory image quality. To improve this, we subsequently propose an alternative method leveraging the equivalent additive property of box-free model watermarking and reverse-engineering a forward surrogate model of HNet, with better image quality preservation. Extensive experimental results on image processing and image generation tasks demonstrate that both attacks achieve impressive watermark removal success rates (100%) while also maintaining excellent image quality (reaching the highest PSNR of 34.69 dB), substantially outperforming existing attacks, highlighting the urgent need for robust defensive strategies to mitigate the identified vulnerability in box-free model watermarking.
#### Summary:
论文提出了一种针对“无框”水印机制（box-free watermarking）的查询式逆向攻击方法。该方法面对的系统由两部分黑盒网络构成：生成网络 GNet 与级联隐藏网络 HNet，二者共同封装为操作网络 ONet，仅对外提供带水印的生成结果。攻击者仅通过向 ONet 发起系列查询即可：1) 先尝试训练一个 HNet 的逆向模型，能在一定程度上恢复无水印图像但画质不佳；2) 转而利用无框水印的“等效可加性”原理，先正向拟合一个 HNet 的替代模型，再用该替代模型从已加水印图像中减去估计的水印，从而以高保真度还原干净图像。在多个图像处理与生成任务上，成功率达 100%，峰值 PSNR 最高 34.69 dB，显著优于现有方法，并指出该漏洞亟需防御。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.18034

### 8. NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN
#### Abstract:
The intellectual property of deep neural network (DNN) models can be protected with DNN watermarking, which embeds copyright watermarks into model parameters (white-box), model behavior (black-box), or model outputs (box-free), and the watermarks can be subsequently extracted to verify model ownership or detect model theft. Despite recent advances, these existing methods are inherently intrusive, as they either modify the model parameters or alter the structure. This natural intrusiveness raises concerns about watermarking-induced shifts in model behavior and the additional cost of fine-tuning, further exacerbated by the rapidly growing model size. As a result, model owners are often reluctant to adopt DNN watermarking in practice, which limits the development of practical Watermarking as a Service (WaaS) systems. To address this issue, we introduce Nonintrusive Watermarking as a Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in which we hypothesize that with the model untouched, an owner-defined watermark can still be extracted from model outputs. Building on this concept, we propose ShadowMark, a concrete implementation of NWaaS which addresses critical deployment challenges by establishing a robust and nonintrusive side channel in the protected model's black-box API, leveraging a key encoder and a watermark decoder. It is significantly distinctive from existing solutions by attaining the so-called absolute fidelity and being applicable to different DNN architectures, while being also robust against existing attacks, eliminating the fidelity-robustness trade-off. Extensive experiments on image-to-image, noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate the efficacy and practicality of ShadowMark for real-world deployment of nonintrusive DNN watermarking.
#### Summary:
本文提出一种面向 X-to-Image 深度生成模型的「非侵入式水印即服务」(NWaaS) 新范式，核心思想是在完全不修改模型参数、结构与训练流程的情况下，仅通过分析模型输出即可提取所有权水印。具体实现方案 ShadowMark 通过在黑盒 API 侧建立「影子信道」：先用密钥编码器为输出图像注入不可见水印，再用水印解码器进行验证。实验表明该方法在多种生成任务（图生图、文生图、噪声生图等）上具备绝对保真度、跨架构通用性，并对现有攻击表现出鲁棒性，避免了传统 DNN 水印面临的保真度-鲁棒性权衡问题。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.18036

### 9. TimelyHLS: LLM-Based Timing-Aware and Architecture-Specific FPGA HLS Optimization
#### Abstract:
Achieving timing closure and design-specific optimizations in FPGA-targeted High-Level Synthesis (HLS) remains a significant challenge due to the complex interaction between architectural constraints, resource utilization, and the absence of automated support for platform-specific pragmas. In this work, we propose TimelyHLS, a novel framework integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to automatically generate and iteratively refine HLS code optimized for FPGA-specific timing and performance requirements. TimelyHLS is driven by a structured architectural knowledge base containing FPGA-specific features, synthesis directives, and pragma templates. Given a kernel, TimelyHLS generates HLS code annotated with both timing-critical and design-specific pragmas. The synthesized RTL is then evaluated using commercial toolchains, and simulation correctness is verified against reference outputs via custom testbenches. TimelyHLS iteratively incorporates synthesis logs and performance reports into the LLM engine for refinement in the presence of functional discrepancies. Experimental results across 10 FPGA architectures and diverse benchmarks show that TimelyHLS reduces the need for manual tuning by up to 70%, while achieving up to 4x latency speedup (e.g., 3.85x for Matrix Multiplication, 3.7x for Bitonic Sort) and over 50% area savings in certain cases (e.g., 57% FF reduction in Viterbi). TimelyHLS consistently achieves timing closure and functional correctness across platforms, highlighting the effectiveness of LLM-driven, architecture-aware synthesis in automating FPGA design.
#### Summary:
TimelyHLS 提出一种基于大语言模型（LLM）与检索增强生成（RAG）的 FPGA HLS 自动化框架。它通过结构化架构知识库（含 FPGA 特性、指令集和 pragma 模板）指导 LLM 为给定内核生成带时序关键 pragma 的 HLS 代码，随后用商业工具链综合并验证功能正确性；综合日志与性能报告被迭代反馈给 LLM 以持续细化。实验覆盖 10 种 FPGA 架构，结果显示相比人工调参减少 70% 工作量，延迟加速最高 4×（矩阵乘法 3.85×、Bitonic Sort 3.7×），面积节省最高 57%。框架稳定达成时序收敛且保证功能正确性，展示了 LLM 驱动的架构感知综合在 FPGA 设计中的潜力。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17962

### 10. Scout: Leveraging Large Language Models for Rapid Digital Evidence Discovery
#### Abstract:
Recent technological advancements and the prevalence of technology in day to day activities have caused a major increase in the likelihood of the involvement of digital evidence in more and more legal investigations. Consumer-grade hardware is growing more powerful, with expanding memory and storage sizes and enhanced processor capabilities. Forensics investigators often have to sift through gigabytes of data during an ongoing investigation making the process tedious. Memory forensics, disk analysis all are well supported by state of the art tools that significantly lower the effort required to be put in by a forensic investigator by providing string searches, analyzing images file etc. During the course of the investigation a lot of false positives are identified that need to be lowered. This work presents Scout, a digital forensics framework that performs preliminary evidence processing and prioritizing using large language models. Scout deploys foundational language models to identify relevant artifacts from a large number of potential evidence files (disk images, captured network packets, memory dumps etc.) which would have taken longer to get identified. Scout employs text based large language models can easily process files with textual information. For the forensic analysis of multimedia files like audio, image, video, office documents etc. multimodal models are employed by Scout. Scout was able to identify and realize the evidence file that were of potential interest for the investigator.
#### Summary:
论文提出一个名为 Scout 的数字取证框架，利用大语言模型（LLM）和多模态模型对磁盘镜像、网络流量包、内存转储等海量证据进行快速初筛与优先级排序，减轻人工分析负担并降低误报。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.18478

### 11. An Improved ChaCha Algorithm Based on Quantum Random Number
#### Abstract:
Due to the merits of high efficiency and strong security against timing and side-channel attacks, ChaCha has been widely applied in real-time communication and data streaming scenarios. However, with the rapid development of AI-assisted cryptanalysis and quantum computing technologies, there are serious challenges to the secure implementation of ChaCha cipher. To further strengthen the security of ChaCha cipher, we propose an improved variant based on quantum random numbers, i.e., Quantum Random Number Enhanced ChaCha (QRE-ChaCha). Specifically, the design XORs the initial constants with quantum random numbers and periodically injects quantum random numbers into selected state words during odd rounds to enhance diffusion. Compared with the original ChaCha, the present variant shows stronger resistance to differential attacks and generates a keystream with statistical randomness, thereby offering increased robustness against both classical and quantum attacks. To evaluate the security and performance of the present ChaCha, our analysis proceeds in three main parts. Firstly, we analyze its theoretical security in terms of quantum randomness and attack testing, and conduct differential cryptanalysis with an automated search method based on the Boolean satisfiability problem (SAT). Secondly, we subject the keystream generated by the cipher to randomness tests using the NIST statistical test suite and the GM/T 0005-2021 randomness testing standard. Finally, we assess its encryption and decryption performance by measuring its encryption speed on files of various sizes. According to the results, the present ChaCha is significantly improved to resist differential attacks while maintaining the high efficiency of the original ChaCha cipher, and its keystream successfully passes statistical randomness tests using the NIST and GM/T 0005-2021 standards, meeting cryptographic application requirements.
#### Summary:
论文提出了一种“量子随机数增强的 ChaCha 流密码”（QRE-ChaCha）。核心创新是将量子随机数引入 ChaCha 的初始化常数和部分轮运算：在奇数轮次把量子随机数注入选定的状态字，以提升扩散并抵抗差分攻击。作者用 SAT 自动差分搜索、NIST 与 GM/T 0005-2021 随机性测试以及性能实测三方面验证：新算法在保持 ChaCha 高效性的同时，通过统计测试，并对经典/量子差分攻击具有更强鲁棒性。
#### Relevance Score: 4.0
#### PDF URL: https://arxiv.org/pdf/2507.18157

### 12. Auto-SGCR: Automated Generation of Smart Grid Cyber Range Using IEC 61850 Standard Models
#### Abstract:
Digitalization of power grids have made them increasingly susceptible to cyber-attacks in the past decade. Iterative cybersecurity testing is indispensable to counter emerging attack vectors and to ensure dependability of critical infrastructure. Furthermore, these can be used to evaluate cybersecurity configuration, effectiveness of the cybersecurity measures against various attack vectors, as well as to train smart grid cybersecurity experts defending the system. Enabling extensive experiments narrows the gap between academic research and production environment. A high-fidelity cyber range is vital as it is often infeasible to conduct such experiments and training using production environment. However, the design and implementation of cyber range requires extensive domain knowledge of physical and cyber aspect of the infrastructure. Furthermore, costs incurred for setup and maintenance of cyber range are significant. Moreover, most existing smart grid cyber ranges are designed as a one-off, proprietary system, and are limited in terms of configurability, accessibility, portability, and reproducibility. To address these challenges, an automated Smart grid Cyber Range generation framework is presented in this paper. Initially a human-/machine-friendly, XML-based modeling language called Smart Grid Modeling Language was defined, which incorporates IEC 61850 System Configuration Language files. Subsequently, a toolchain to parse SG-ML model files and automatically instantiate a functional smart grid cyber range was developed. The developed SG-ML models can be easily shared and/or modified to reproduce or customize for any cyber range. The application of Auto-SGCR is demonstrated through case studies with large-scale substation models. The toolchain along with example SG-ML models have been open-sourced.
#### Summary:
本文提出 Auto-SGCR——一套自动化生成智能电网网络靶场的框架。作者首先定义了基于 XML 的 Smart Grid Modeling Language（SG-ML），可直接嵌入 IEC 61850 系统配置文件；随后开发了工具链，用于解析 SG-ML 并自动实例化高保真、可复现、可定制的智能电网网络靶场。案例研究验证了在大型变电站模型上的可扩展性，相关代码与模型已开源，旨在降低靶场构建成本，弥合学术研究与生产环境差距。
#### Relevance Score: 4.0
#### PDF URL: https://arxiv.org/pdf/2507.18249

### 13. Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment
#### Abstract:
The deployment of large-scale software-based 5G core functions presents significant challenges due to their reliance on optimized and intelligent resource provisioning for their services. Many studies have focused on analyzing the impact of resource allocation for complex deployments using mathematical models, queue theories, or even Artificial Intelligence (AI). This paper elucidates the effects of chaotic workloads, generated by Distributed Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment registration performance. Our findings highlight the necessity of diverse resource profiles to ensure Service-Level Agreement (SLA) compliance in large-scale 5G core deployments. Additionally, our analysis of packet capture approaches demonstrates the potential of kernel-based monitoring for scalable security threat defense. Finally, our empirical evaluation provides insights into the effective deployment of 5G NFs in complex scenarios.
#### Summary:
本文聚焦于大规模软件化5G核心网在真实部署中的性能评估与威胁缓解。作者系统分析了由分布式拒绝服务（DDoS）引发的混沌负载对各类网络功能（NFs）以及用户设备注册性能的影响；指出为保障服务等级协议（SLA）在大规模场景下的合规性，必须为不同NF配备差异化的资源配置画像。此外，论文探讨了基于内核的数据包捕获方法，验证其在可扩展的安全威胁防御中的潜力，并通过实测给出了在复杂场景下部署5G NF的经验结论。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17850

### 14. Formal Verification of the Safegcd Implementation
#### Abstract:
The modular inverse is an essential piece of computation required for elliptic curve operations used for digital signatures in Bitcoin and other applications. A novel approach to the extended Euclidean algorithm has been developed by Bernstein and Yang within the last few years and incorporated into the libsecp256k1 cryptographic library used by Bitcoin. However, novel algorithms introduce new risks of errors. To address this we have completed a computer verified proof of the correctness of (one of) libsecp256k1's modular inverse implementations with the Coq proof assistant using the Verifiable C's implementation of separation logic.
#### Summary:
本文使用 Coq 证明辅助工具与 Verifiable C 的分离逻辑，首次为 Bitcoin 核心库 libsecp256k1 中基于 Bernstein-Yang 改进版扩展欧几里得算法的 safegcd 模逆实现给出了机器可验证的形式化正确性证明，从而排除因算法新颖而可能引入的实现错误。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17956

### 15. MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection
#### Abstract:
Phishing emails continue to pose a significant threat to cybersecurity by exploiting human vulnerabilities through deceptive content and malicious payloads. While Machine Learning (ML) models are effective at detecting phishing threats, their performance largely relies on the quality and diversity of the training data. This paper presents MeAJOR (Merged email Assets from Joint Open-source Repositories) Corpus, a novel, multi-source phishing email dataset designed to overcome critical limitations in existing resources. It integrates 135894 samples representing a broad number of phishing tactics and legitimate emails, with a wide spectrum of engineered features. We evaluated the dataset's utility for phishing detection research through systematic experiments with four classification models (RF, XGB, MLP, and CNN) across multiple feature configurations. Results highlight the dataset's effectiveness, achieving 98.34% F1 with XGB. By integrating broad features from multiple categories, our dataset provides a reusable and consistent resource, while addressing common challenges like class imbalance, generalisability and reproducibility.
#### Summary:
本文构建了名为 MeAJOR 的多源钓鱼邮件数据集，共 135 894 封邮件，涵盖多种钓鱼手法与正常邮件，并提供丰富的手工特征。作者在 RF、XGB、MLP、CNN 四类模型上系统实验，XGB 在数据集上达到 98.34% 的 F1，验证了数据集的多样性、可复现性与实用性，可用于后续钓鱼检测研究。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.17978

### 16. Conformidade com os Requisitos Legais de Privacidade de Dados: Um Estudo sobre Técnicas de Anonimização
#### Abstract:
The protection of personal data has become a central topic in software development, especially with the implementation of the General Data Protection Law (LGPD) in Brazil and the General Data Protection Regulation (GDPR) in the European Union. With the enforcement of these laws, certain software quality criteria have become mandatory, such as data anonymization, which is one of the main aspects addressed by these regulations. The aim of this article is to analyze data anonymization techniques and assess their effectiveness in ensuring compliance with legal requirements and the utility of the data for its intended purpose. Techniques such as aggregation, generalization, perturbation, and k-anonymity were investigated and applied to datasets containing personal and sensitive data. The analysis revealed significant variations in the effectiveness of each method, highlighting the need to balance privacy and data utility.
#### Summary:
本文系统梳理了在巴西 LGPD 与欧盟 GDPR 框架下的主流数据匿名化技术（聚合、泛化、扰动、k-匿名），通过实验对比它们在兼顾隐私合规与数据可用性方面的效果差异，指出不同方法在隐私-效用平衡上存在显著差距，需要针对具体场景做权衡。
#### Relevance Score: 3.0
#### PDF URL: https://arxiv.org/pdf/2507.18360

