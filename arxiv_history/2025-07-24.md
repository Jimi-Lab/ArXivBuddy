# Daily arXiv Papers
## Date: 2025-07-24
## Description: I am working on the research area of computer vision and natural language processing. 
Specifically, I am interested in the following fieds:
1. Object detection
2. AIGC (AI Generated Content)
3. Multimodal Large Language Models

I'm not interested in the following fields:
1. 3D Vision
2. Robotics
3. Low-level Vision

## Papers:
### 1. SIA: Enhancing Safety via Intent Awareness for Vision-Language Models
#### Abstract:
As vision-language models (VLMs) are increasingly deployed in real-world applications, new safety risks arise from the subtle interplay between images and text. In particular, seemingly innocuous inputs can combine to reveal harmful intent, leading to unsafe model responses. Despite increasing attention to multimodal safety, previous approaches based on post hoc filtering or static refusal prompts struggle to detect such latent risks, especially when harmfulness emerges only from the combination of inputs. We propose SIA (Safety via Intent Awareness), a training-free prompt engineering framework that proactively detects and mitigates harmful intent in multimodal inputs. SIA employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement. Rather than relying on predefined rules or classifiers, SIA dynamically adapts to the implicit intent inferred from the image-text pair. Through extensive experiments on safety-critical benchmarks including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves substantial safety improvements, outperforming prior methods. Although SIA shows a minor reduction in general reasoning accuracy on MMStar, the corresponding safety gains highlight the value of intent-aware reasoning in aligning VLMs with human-centric values.
#### Summary:
这篇论文提出了SIA（Safety via Intent Awareness）框架，旨在通过意图感知增强视觉语言模型（VLMs）的安全性。SIA采用三阶段推理过程：视觉抽象、意图推断和意图条件响应细化，以动态检测和缓解多模态输入中的有害意图。实验表明，SIA在多个安全关键基准测试中显著提升了安全性，尽管在一般推理准确性上略有下降。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.16856

### 2. Pixels, Patterns, but No Poetry: To See The World like Humans
#### Abstract:
Achieving human-like perception and reasoning in Multimodal Large Language Models (MLLMs) remains a central challenge in artificial intelligence. While recent research has primarily focused on enhancing reasoning capabilities in MLLMs, a fundamental question persists: Can Multimodal Large Language Models truly perceive the world as humans do? This paper shifts focus from reasoning to perception. Rather than constructing benchmarks specifically for reasoning, we introduce the Turing Eye Test (TET), a challenging perception-oriented benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on synthetic images that humans process intuitively. Our findings reveal that state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks trivial for humans. Both in-context learning and training on language backbone-effective for previous benchmarks-fail to improve performance on our tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting that our benchmark poses challenges for vision tower generalization rather than for the knowledge and reasoning capabilities of the language backbone-a key gap between current MLLMs and human perception. We release a representative subset of TET tasks in this version, and will introduce more diverse tasks and methods to enhance visual generalization in future work.
#### Summary:
这篇论文探讨了多模态大语言模型（MLLMs）是否能够像人类一样感知世界的问题。作者提出了一个名为图灵眼测试（TET）的感知导向基准，包含四个诊断任务，用于评估MLLMs在合成图像上的表现。研究发现，当前最先进的MLLMs在这些对人类来说简单的感知任务上表现糟糕，而微调视觉模块可以显著提升性能，表明视觉模块的泛化能力是MLLMs与人类感知之间的关键差距。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.16863

### 3. Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed
#### Abstract:
Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI.
#### Summary:
本文研究了文本到图像扩散模型（DMs）中的数据记忆问题，挑战了现有关于记忆局部性的假设。研究发现，即使通过剪枝方法移除被认为触发数据复制的权重，轻微调整输入提示的文本嵌入仍可重新触发复制行为，表明现有防御措施的脆弱性。此外，研究还表明，复制可以从文本嵌入空间的不同位置触发，并遵循不同的模型路径，进一步质疑了记忆局部性的基本假设。作为解决方案的第一步，作者提出了一种新颖的对抗性微调方法，通过迭代搜索复制触发器并更新模型以提高鲁棒性。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.16880

### 4. FedVLM: Scalable Personalized Vision-Language Models through Federated Learning
#### Abstract:
Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot learning capabilities, making them essential for several downstream tasks. However, fine-tuning these models at scale remains challenging, particularly in federated environments where data is decentralized and non-iid across clients. Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation) reduce computational overhead but struggle with heterogeneous client data, leading to suboptimal generalization. To address these challenges, we propose FedVLM, a federated LoRA fine-tuning framework that enables decentralized adaptation of VLMs while preserving model privacy and reducing reliance on centralized training. To further tackle data heterogeneity, we introduce personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each client's unique data distribution, significantly improving local adaptation while maintaining global model aggregation. Experiments on the RLAIF-V dataset show that pLoRA improves client-specific performance by 24.5% over standard LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a scalable and efficient solution for fine-tuning VLMs in federated settings, advancing personalized adaptation in distributed learning scenarios.
#### Summary:
这篇论文提出了FedVLM，一个基于联邦学习的视觉-语言模型（VLM）微调框架，旨在解决在数据分散且非独立同分布（non-iid）的联邦环境中微调VLMs的挑战。通过引入个性化的LoRA（pLoRA）方法，FedVLM能够动态适应每个客户端的数据分布，显著提高了局部适应性，同时在全局模型聚合中保持了性能。实验表明，pLoRA在非iid设置下比标准LoRA提高了24.5%的客户端特定性能。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17088

### 5. Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment
#### Abstract:
The quality assessment of AI-generated content (AIGC) faces multi-dimensional challenges, that span from low-level visual perception to high-level semantic understanding. Existing methods generally rely on single-level visual features, limiting their ability to capture complex distortions in AIGC images. To address this limitation, a multi-level visual representation paradigm is proposed with three stages, namely multi-level feature extraction, hierarchical fusion, and joint aggregation. Based on this paradigm, two networks are developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net) is designed for the perceptual quality assessment, extracting complementary local and global features via dual CNN and Transformer visual backbones. The Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image correspondence by embedding prompt semantics into the visual feature fusion process at each feature level. The fused multi-level features are then aggregated for final evaluation. Experiments on benchmarks demonstrate outstanding performance on both tasks, validating the effectiveness of the proposed multi-level visual assessment paradigm.
#### Summary:
这篇论文提出了一种用于AIGC图像质量评估的多级特征表示方法，包括多级特征提取、分层融合和联合聚合三个阶段。作者开发了两个网络：MGLF-Net用于感知质量评估，通过CNN和Transformer双视觉骨干提取局部和全局特征；MPEF-Net针对文本到图像对应性，将提示语义嵌入到每个特征级别的视觉特征融合过程中。实验验证了该多级视觉评估范式的有效性。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17182

### 6. Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models
#### Abstract:
In this paper, we introduce VideoNarrator, a novel training-free pipeline designed to generate dense video captions that offer a structured snapshot of video content. These captions offer detailed narrations with precise timestamps, capturing the nuances present in each segment of the video. Despite advancements in multimodal large language models (MLLMs) for video comprehension, these models often struggle with temporally aligned narrations and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator addresses these challenges by leveraging a flexible pipeline where off-the-shelf MLLMs and visual-language models (VLMs) can function as caption generators, context providers, or caption verifiers. Our experimental results demonstrate that the synergistic interaction of these components significantly enhances the quality and accuracy of video narrations, effectively reducing hallucinations and improving temporal alignment. This structured approach not only enhances video understanding but also facilitates downstream tasks such as video summarization and video question answering, and can be potentially extended for advertising and marketing applications.
#### Summary:
本文介绍了VideoNarrator，一种无需训练的流程，旨在生成密集的视频字幕，提供视频内容的结构化快照。这些字幕提供带有精确时间戳的详细叙述，捕捉视频每个片段的细微差别。尽管多模态大型语言模型（MLLMs）在视频理解方面取得了进展，但这些模型往往难以实现时间对齐的叙述，并在不熟悉的场景中产生幻觉。VideoNarrator通过灵活的流程解决了这些挑战，其中现成的MLLMs和视觉语言模型（VLMs）可以作为字幕生成器、上下文提供者或字幕验证器。实验结果表明，这些组件的协同作用显著提高了视频叙述的质量和准确性，有效减少了幻觉并改善了时间对齐。这种结构化方法不仅增强了视频理解，还促进了视频摘要和视频问答等下游任务，并可能扩展到广告和营销应用中。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17050

### 7. MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training
#### Abstract:
Foundation models have recently gained tremendous popularity in medical image analysis. State-of-the-art methods leverage either paired image-text data via vision-language pre-training or unpaired image data via self-supervised pre-training to learn foundation models with generalizable image features to boost downstream task performance. However, learning foundation models exclusively on either paired or unpaired image data limits their ability to learn richer and more comprehensive image features. In this paper, we investigate a novel task termed semi-supervised vision-language pre-training, aiming to fully harness the potential of both paired and unpaired image data for foundation model learning. To this end, we propose MaskedCLIP, a synergistic masked image modeling and contrastive language-image pre-training framework for semi-supervised vision-language pre-training. The key challenge in combining paired and unpaired image data for learning a foundation model lies in the incompatible feature spaces derived from these two types of data. To address this issue, we propose to connect the masked feature space with the CLIP feature space with a bridge transformer. In this way, the more semantic specific CLIP features can benefit from the more general masked features for semantic feature extraction. We further propose a masked knowledge distillation loss to distill semantic knowledge of original image features in CLIP feature space back to the predicted masked image features in masked feature space. With this mutually interactive design, our framework effectively leverages both paired and unpaired image data to learn more generalizable image features for downstream tasks. Extensive experiments on retinal image analysis demonstrate the effectiveness and data efficiency of our method.
#### Summary:
这篇论文提出了MaskedCLIP，一个结合掩码图像建模和对比语言-图像预训练的框架，用于半监督视觉-语言预训练。该方法旨在充分利用配对和非配对图像数据来学习更具泛化能力的图像特征，通过桥接变换器将掩码特征空间与CLIP特征空间连接起来，并提出了掩码知识蒸馏损失来增强语义特征提取。在视网膜图像分析上的实验验证了该方法的有效性和数据效率。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17239

### 8. Principled Multimodal Representation Learning
#### Abstract:
Multimodal representation learning seeks to create a unified representation space by integrating diverse data modalities to improve multimodal understanding. Traditional methods often depend on pairwise contrastive learning, which relies on a predefined anchor modality, restricting alignment across all modalities. Recent advances have investigated the simultaneous alignment of multiple modalities, yet several challenges remain, such as limitations imposed by fixed anchor points and instability arising from optimizing the product of singular values. To address the challenges, in this paper, we propose Principled Multimodal Representation Learning (PMRL), a novel framework that achieves simultaneous alignment of multiple modalities without anchor dependency in a more stable manner. Specifically, grounded in the theoretical insight that full alignment corresponds to a rank-1 Gram matrix, PMRL optimizes the dominant singular value of the representation matrix to align modalities along a shared leading direction. We propose a softmax-based loss function that treats singular values as logits to prioritize the largest singular value. Besides, instance-wise contrastive regularization on the leading eigenvectors maintains inter-instance separability and prevents representation collapse. Extensive experiments across diverse tasks demonstrate PMRL's superiority compared to baseline methods. The source code will be publicly available.
#### Summary:
这篇论文提出了一种名为Principled Multimodal Representation Learning (PMRL)的新框架，旨在通过优化表示矩阵的主导奇异值，实现多模态的无锚点依赖对齐。PMRL基于理论洞察，即完全对齐对应于一个秩-1的Gram矩阵，通过软最大化损失函数和实例对比正则化，解决了传统方法中固定锚点限制和优化不稳定的问题。实验表明，PMRL在多种任务上优于基线方法。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17343

### 9. SFUOD: Source-Free Unknown Object Detection
#### Abstract:
Source-free object detection adapts a detector pre-trained on a source domain to an unlabeled target domain without requiring access to labeled source data. While this setting is practical as it eliminates the need for the source dataset during domain adaptation, it operates under the restrictive assumption that only pre-defined objects from the source domain exist in the target domain. This closed-set setting prevents the detector from detecting undefined objects. To ease this assumption, we propose Source-Free Unknown Object Detection (SFUOD), a novel scenario which enables the detector to not only recognize known objects but also detect undefined objects as unknown objects. To this end, we propose CollaPAUL (Collaborative tuning and Principal Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning enhances knowledge adaptation by integrating target-dependent knowledge from the auxiliary encoder with source-dependent knowledge from the pre-trained detector through a cross-domain attention mechanism. Additionally, principal axes-based unknown labeling assigns pseudo-labels to unknown objects by estimating objectness via principal axes projection and confidence scores from model predictions. The proposed CollaPAUL achieves state-of-the-art performances on SFUOD benchmarks, and extensive experiments validate its effectiveness.
#### Summary:
这篇论文提出了SFUOD（Source-Free Unknown Object Detection）方法，旨在解决源自由目标检测中无法检测未定义对象的问题。作者提出了CollaPAUL框架，通过协作调整和基于主轴的未知标记技术，实现了在无源数据情况下检测已知和未知对象的能力。该方法在SFUOD基准测试中达到了最先进的性能。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17373

### 10. HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs
#### Abstract:
Video Anomaly Detection (VAD) aims to identify and locate deviations from normal patterns in video sequences. Traditional methods often struggle with substantial computational demands and a reliance on extensive labeled datasets, thereby restricting their practical applicability. To address these constraints, we propose HiProbe-VAD, a novel framework that leverages pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring fine-tuning. In this paper, we discover that the intermediate hidden states of MLLMs contain information-rich representations, exhibiting higher sensitivity and linear separability for anomalies compared to the output layer. To capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP) mechanism that intelligently identifies and extracts the most informative hidden states from the optimal intermediate layer during the MLLMs reasoning. Then a lightweight anomaly scorer and temporal localization module efficiently detects anomalies using these extracted hidden states and finally generate explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate that HiProbe-VAD outperforms existing training-free and most traditional approaches. Furthermore, our framework exhibits remarkable cross-model generalization capabilities in different MLLMs without any tuning, unlocking the potential of pre-trained MLLMs for video anomaly detection and paving the way for more practical and scalable solutions.
#### Summary:
这篇论文提出了HiProbe-VAD，一种利用预训练多模态大语言模型（MLLMs）进行视频异常检测（VAD）的新框架。该框架无需微调，通过动态层显著性探测（DLSP）机制智能地提取MLLMs中间隐藏状态中的信息丰富表示，用于异常检测和定位。实验表明，HiProbe-VAD在UCF-Crime和XD-Violence数据集上优于现有的免训练和大多数传统方法，并展示了跨模型的泛化能力。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17394

### 11. Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection
#### Abstract:
The Mixture of Experts (MoE) architecture has excelled in Large Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary object detectors, which also leverage large-scale vision-language datasets but smaller models, remains unexplored. This work investigates this domain, revealing intriguing insights. In the shallow layers, experts tend to cooperate with diverse peers to expand the search space. While in the deeper layers, fixed collaborative structures emerge, where each expert maintains 2-3 fixed partners and distinct expert combinations are specialized in processing specific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding DINO 1.5 Edge from a dense model to a dynamic inference framework via an efficient MoE-Tuning strategy. Additionally, we design a granularity decomposition mechanism to decompose the Feed-Forward Network (FFN) of base model into multiple smaller expert networks, expanding the subnet search space. To prevent performance degradation at the start of fine-tuning, we further propose a pre-trained weight allocation strategy for the experts, coupled with a specific router initialization. During inference, only the input-relevant experts are activated to form a compact subnet. Experiments show that, pretrained with merely 1.56M open-source data, Dynamic-DINO outperforms Grounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.
#### Summary:
这篇论文提出了Dynamic-DINO，一种基于专家混合（MoE）架构的实时开放词汇目标检测方法。该方法通过高效的MoE调优策略，将Grounding DINO 1.5 Edge从密集模型扩展为动态推理框架，并设计了粒度分解机制和预训练权重分配策略，以优化专家网络的性能。实验表明，Dynamic-DINO在仅使用1.56M开源数据预训练的情况下，性能优于使用私有数据集预训练的Grounding DINO 1.5 Edge。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17436

### 12. Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls
#### Abstract:
This study investigates the extent to which the Visual Entailment (VE) task serves as a reliable probe of vision-language understanding in multimodal language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond reporting performance metrics, we aim to interpret what these results reveal about the underlying possibilities and limitations of the VE task. We conduct a series of experiments across zero-shot, few-shot, and fine-tuning settings, exploring how factors such as prompt design, the number and order of in-context examples and access to visual information might affect VE performance. To further probe the reasoning processes of the model, we used explanation-based evaluations. Results indicate that three-shot inference outperforms the zero-shot baselines. However, additional examples introduce more noise than they provide benefits. Additionally, the order of the labels in the prompt is a critical factor that influences the predictions. In the absence of visual information, the model has a strong tendency to hallucinate and imagine content, raising questions about the model's over-reliance on linguistic priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model. Additionally, the explanation evaluation demonstrates that the fine-tuned model provides semantically meaningful explanations similar to those of humans, with a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore results in experiments with limited vision, questioning the visual grounding of this task. Overall, our results highlight both the utility and limitations of VE as a diagnostic task for vision-language understanding and point to directions for refining multimodal evaluation methods.
#### Summary:
本研究探讨了视觉蕴含（VE）任务作为多模态语言模型中视觉-语言理解可靠探针的程度，使用LLaMA 3.2 11B Vision模型作为测试案例。研究通过零样本、少样本和微调设置进行了一系列实验，分析了提示设计、上下文示例数量与顺序以及视觉信息访问对VE性能的影响。结果显示，三样本推理优于零样本基线，但更多示例会引入噪声。标签顺序对预测有显著影响，且缺乏视觉信息时模型容易产生幻觉。微调后模型在e-SNLI-VE数据集上达到83.3%的准确率，优于现有技术OFA-X模型。解释评估显示微调模型能提供语义上有意义的解释，但视觉基础的可靠性仍存疑。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17467

### 13. Dual-branch Prompting for Multimodal Machine Translation
#### Abstract:
Multimodal Machine Translation (MMT) typically enhances text-only translation by incorporating aligned visual features. Despite the remarkable progress, state-of-the-art MMT approaches often rely on paired image-text inputs at inference and are sensitive to irrelevant visual noise, which limits their robustness and practical applicability. To address these issues, we propose D2P-MMT, a diffusion-based dual-branch prompting framework for robust vision-guided translation. Specifically, D2P-MMT requires only the source text and a reconstructed image generated by a pre-trained diffusion model, which naturally filters out distracting visual details while preserving semantic cues. During training, the model jointly learns from both authentic and reconstructed images using a dual-branch prompting strategy, encouraging rich cross-modal interactions. To bridge the modality gap and mitigate training-inference discrepancies, we introduce a distributional alignment loss that enforces consistency between the output distributions of the two branches. Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves superior translation performance compared to existing state-of-the-art approaches.
#### Summary:
这篇论文提出了一种名为D2P-MMT的双分支提示框架，用于鲁棒的多模态机器翻译（MMT）。该方法通过使用预训练的扩散模型生成重建图像，过滤掉无关的视觉噪声，同时保留语义线索。在训练过程中，模型通过双分支提示策略从真实和重建图像中学习，促进丰富的跨模态交互。此外，还引入了分布对齐损失以减少训练和推理之间的差异。实验表明，D2P-MMT在Multi30K数据集上优于现有最先进方法。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17588

### 14. See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering
#### Abstract:
Multimodal Large Language Models (MLLMs) have pushed the frontiers of Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is fundamentally bottlenecked by a reliance on uni-dimensional evidence. This "seeing only the trees, but not the forest" approach prevents robust, multi-faceted understanding. Inspired by the principle of seeing both the forest and trees, we propose Synergos-VQA, a novel synergistic reasoning framework. At its core, Synergos-VQA concurrently generates and fuses three complementary evidence streams at inference time: (1) Holistic Evidence to perceive the entire scene (the "forest"), (2) Structural Evidence from a prototype-driven module to identify key objects (the "trees"), and (3) Causal Evidence from a counterfactual probe to ensure the reasoning is robustly grounded. By synergistically fusing this multi-faceted evidence, our framework achieves a more comprehensive and reliable reasoning process. Extensive experiments show that Synergos-VQA decisively establishes a new state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA. Furthermore, our approach demonstrates strong plug-and-play capabilities, significantly boosting various open-source MLLMs and proving that superior methodological design can outperform sheer model scale.
#### Summary:
这篇论文提出了Synergos-VQA，一个新颖的协同推理框架，用于基于知识的视觉问答（KBVQA）。该框架通过同时生成和融合三种互补的证据流（整体证据、结构证据和因果证据），实现了更全面和可靠的推理过程。实验表明，Synergos-VQA在多个挑战性基准测试中取得了新的最先进性能，并展示了强大的即插即用能力。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17659

### 15. Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning
#### Abstract:
Multimodal large language models (MLLMs) demonstrate significant potential in the field of medical diagnosis. However, they face critical challenges in specialized domains such as ophthalmology, particularly the fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding. This paper introduces FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed through the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within a single fundus image. Additionally, by constructing a clinically aligned cognitive chain, it guides the model to generate interpretable reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen, achieves the best performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in zero-shot report generation tasks, achieving a clinical consistency of 77.0%, significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling law between data quality and model capability ($L \propto N^{0.068}$), demonstrating that the cognitive alignment annotations in FundusGen enhance data utilization efficiency. By integrating region-level localization with diagnostic reasoning chains, our work develops a scalable, clinically-aligned MLLM and explores a pathway toward bridging the visual-language gap in specific MLLMs. Our project can be found at this https URL.
#### Summary:
这篇论文提出了FundusExpert，一种专用于眼科的多模态大语言模型（MLLM），通过整合定位-诊断推理能力和临床认知链推理，解决了眼科领域中的注释粒度碎片化和临床推理逻辑不一致的问题。论文还介绍了FundusGen数据集，通过智能Fundus-Engine系统自动生成，结合了全局疾病分类、局部目标检测和细粒度特征分析。实验表明，FundusExpert在眼科问答任务和零样本报告生成任务中表现优异，显著优于现有模型。此外，论文还揭示了数据质量与模型能力之间的缩放规律。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17539

### 16. Filter-And-Refine: A MLLM Based Cascade System for Industrial-Scale Video Content Moderation
#### Abstract:
Effective content moderation is essential for video platforms to safeguard user experience and uphold community standards. While traditional video classification models effectively handle well-defined moderation tasks, they struggle with complicated scenarios such as implicit harmful content and contextual ambiguity. Multimodal large language models (MLLMs) offer a promising solution to these limitations with their superior cross-modal reasoning and contextual understanding. However, two key challenges hinder their industrial adoption. First, the high computational cost of MLLMs makes full-scale deployment impractical. Second, adapting generative models for discriminative classification remains an open research problem. In this paper, we first introduce an efficient method to transform a generative MLLM into a multimodal classifier using minimal discriminative training data. To enable industry-scale deployment, we then propose a router-ranking cascade system that integrates MLLMs with a lightweight router model. Offline experiments demonstrate that our MLLM-based approach improves F1 score by 66.50% over traditional classifiers while requiring only 2% of the fine-tuning data. Online evaluations show that our system increases automatic content moderation volume by 41%, while the cascading deployment reduces computational cost to only 1.5% of direct full-scale deployment.
#### Summary:
这篇论文提出了一种基于多模态大语言模型（MLLM）的级联系统，用于工业级视频内容审核。论文首先介绍了一种高效方法，将生成式MLLM转化为多模态分类器，仅需少量判别性训练数据。然后，提出了一种路由器-排序级联系统，将MLLM与轻量级路由器模型结合，以降低计算成本。实验表明，该方法在F1分数上比传统分类器提高了66.50%，同时仅需2%的微调数据。在线评估显示，该系统将自动内容审核量提高了41%，而级联部署将计算成本降低至直接全规模部署的1.5%。
#### Relevance Score: 8.0
#### PDF URL: https://arxiv.org/pdf/2507.17204

### 17. ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension
#### Abstract:
Referring Expression Comprehension (REC) aims to localize specified entities or regions in an image based on natural language descriptions. While existing methods handle single-entity localization, they often ignore complex inter-entity relationships in multi-entity scenes, limiting their accuracy and reliability. Additionally, the lack of high-quality datasets with fine-grained, paired image-text-relation annotations hinders further progress. To address this challenge, we first construct a relation-aware, multi-entity REC dataset called ReMeX, which includes detailed relationship and textual annotations. We then propose ReMeREC, a novel framework that jointly leverages visual and textual cues to localize multiple entities while modeling their inter-relations. To address the semantic ambiguity caused by implicit entity boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron (TMP), which dynamically infers both the quantity and span of entities from fine-grained textual cues, producing distinctive representations. Additionally, our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and global scene understanding. To further improve language comprehension for fine-grained prompts, we also construct a small-scale auxiliary dataset, EntityText, generated using large language models. Experiments on four benchmark datasets show that ReMeREC achieves state-of-the-art performance in multi-entity grounding and relation prediction, outperforming existing approaches by a large margin.
#### Summary:
这篇论文提出了ReMeREC框架，旨在解决多实体指代表达理解（REC）中的复杂实体间关系问题。作者首先构建了一个包含细粒度图像-文本-关系标注的数据集ReMeX，并提出了Text-adaptive Multi-entity Perceptron（TMP）和Entity Inter-relationship Reasoner（EIR）来动态推断实体数量与范围，并增强关系推理。此外，还利用大语言模型生成了辅助数据集EntityText。实验表明，ReMeREC在多实体定位和关系预测上达到了最先进的性能。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.16877

### 18. CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis
#### Abstract:
Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect terms within paired image-text data and determine their fine grained sentiment polarities, representing a fundamental task for improving the effectiveness of applications such as product review systems and public opinion monitoring. Existing methods face challenges such as cross modal alignment noise and insufficient consistency in fine-grained representations. While global modality alignment methods often overlook the connection between aspect terms and their corresponding local visual regions, bridging the representation gap between text and images remains a challenge. To address these limitations, this paper introduces an end to end Contrastive Learning framework with Adaptive Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed of three novel modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive Attention Fusion network enhances fine-grained alignment between textual features and image regions via hierarchical, multi-stage cross modal interactions, effectively suppressing irrelevant visual noise. Secondly, multi-task contrastive learning combines global modal contrast and local granularity alignment to enhance cross modal representation consistency. Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting mechanism to calibrate loss contributions according to each task's uncertainty, thereby mitigating gradient interference. Evaluation on standard public benchmarks demonstrates that CLAMP consistently outperforms the vast majority of existing state of the art methods.
#### Summary:
这篇论文提出了一种名为CLAMP的端到端对比学习框架，用于多模态基于方面的情感分析（MABSA）。该框架包含三个新模块：渐进注意力融合网络、多任务对比学习和自适应多损失聚合。渐进注意力融合网络通过分层次、多阶段的跨模态交互，增强了文本特征与图像区域之间的细粒度对齐，有效抑制了无关的视觉噪声。多任务对比学习结合了全局模态对比和局部粒度对齐，以增强跨模态表示的一致性。自适应多损失聚合采用基于动态不确定性的加权机制，根据每个任务的不确定性校准损失贡献，从而减轻梯度干扰。在标准公共基准上的评估表明，CLAMP在大多数现有最先进方法中表现优异。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.16854

### 19. Controllable Hybrid Captioner for Improved Long-form Video Understanding
#### Abstract:
Video data, especially long-form video, is extremely dense and high-dimensional. Text-based summaries of video content offer a way to represent query-relevant content in a much more compact manner than raw video. In addition, textual representations are easily ingested by state-of-the-art large language models (LLMs), which enable reasoning over video content to answer complex natural language queries. To solve this issue, we rely on the progressive construction of a text-based memory by a video captioner operating on shorter chunks of the video, where spatio-temporal modeling is computationally feasible. We explore ways to improve the quality of the activity log comprised solely of short video captions. Because the video captions tend to be focused on human actions, and questions may pertain to other information in the scene, we seek to enrich the memory with static scene descriptions using Vision Language Models (VLMs). Our video understanding system relies on the LaViLa video captioner in combination with a LLM to answer questions about videos. We first explored different ways of partitioning the video into meaningful segments such that the textual descriptions more accurately reflect the structure of the video content. Furthermore, we incorporated static scene descriptions into the captioning pipeline using LLaVA VLM, resulting in a more detailed and complete caption log and expanding the space of questions that are answerable from the textual memory. Finally, we have successfully fine-tuned the LaViLa video captioner to produce both action and scene captions, significantly improving the efficiency of the captioning pipeline compared to using separate captioning models for the two tasks. Our model, controllable hybrid captioner, can alternate between different types of captions according to special input tokens that signals scene changes detected in the video.
#### Summary:
这篇论文提出了一种可控混合字幕生成器，用于提升长视频的理解能力。通过结合视频字幕生成器（LaViLa）和大型语言模型（LLM），该系统能够生成更详细的视频内容文本摘要，并回答关于视频的复杂自然语言查询。论文探讨了如何通过分割视频片段和使用视觉语言模型（VLM）如LLaVA来丰富静态场景描述，从而提升字幕日志的质量和完整性。此外，作者还微调了LaViLa模型，使其能够根据输入标记交替生成动作和场景描述，显著提高了字幕生成效率。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17047

### 20. Perceptual Classifiers: Detecting Generative Images using Perceptual Features
#### Abstract:
Image Quality Assessment (IQA) models are employed in many practical image and video processing pipelines to reduce storage, minimize transmission costs, and improve the Quality of Experience (QoE) of millions of viewers. These models are sensitive to a diverse range of image distortions and can accurately predict image quality as judged by human viewers. Recent advancements in generative models have resulted in a significant influx of "GenAI" content on the internet. Existing methods for detecting GenAI content have progressed significantly with improved generalization performance on images from unseen generative models. Here, we leverage the capabilities of existing IQA models, which effectively capture the manifold of real images within a bandpass statistical space, to distinguish between real and AI-generated images. We investigate the generalization ability of these perceptual classifiers to the task of GenAI image detection and evaluate their robustness against various image degradations. Our results show that a two-layer network trained on the feature space of IQA models demonstrates state-of-the-art performance in detecting fake images across generative models, while maintaining significant robustness against image degradations.
#### Summary:
这篇论文探讨了利用图像质量评估（IQA）模型的感知特征来检测生成式AI（GenAI）图像的方法。作者发现，IQA模型能够有效捕捉真实图像的统计特征，从而区分真实图像与AI生成图像。通过在两层级网络上训练IQA模型的特征空间，该方法在检测不同生成模型的假图像方面表现出色，并且对图像退化具有显著的鲁棒性。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17240

### 21. VisionTrap: Unanswerable Questions On Visual Data
#### Abstract:
Visual Question Answering (VQA) has been a widely studied topic, with extensive research focusing on how VLMs respond to answerable questions based on real-world images. However, there has been limited exploration of how these models handle unanswerable questions, particularly in cases where they should abstain from providing a response. This research investigates VQA performance on unrealistically generated images or asking unanswerable questions, assessing whether models recognize the limitations of their knowledge or attempt to generate incorrect answers. We introduced a dataset, VisionTrap, comprising three categories of unanswerable questions across diverse image types: (1) hybrid entities that fuse objects and animals, (2) objects depicted in unconventional or impossible scenarios, and (3) fictional or non-existent figures. The questions posed are logically structured yet inherently unanswerable, testing whether models can correctly recognize their limitations. Our findings highlight the importance of incorporating such questions into VQA benchmarks to evaluate whether models tend to answer, even when they should abstain.
#### Summary:
这篇论文《VisionTrap: Unanswerable Questions On Visual Data》研究了视觉问答（VQA）模型在处理不可回答问题时的表现。作者创建了一个名为VisionTrap的数据集，包含三类不可回答的问题：融合物体和动物的混合实体、物体在不合理场景中的描绘以及虚构或不存在的角色。论文探讨了模型在面对这些问题时是否会错误地生成答案，而不是正确识别其知识限制。研究强调了在VQA基准测试中加入此类问题的重要性，以评估模型是否会在应该避免回答时仍试图提供答案。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17262

### 22. Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection
#### Abstract:
Human-Object Interaction (HOI) detection aims to identify humans and objects within images and interpret their interactions. Existing HOI methods rely heavily on large datasets with manual annotations to learn interactions from visual cues. These annotations are labor-intensive to create, prone to inconsistency, and limit scalability to new domains and rare interactions. We argue that recent advances in Vision-Language Models (VLMs) offer untapped potential, particularly in enhancing interaction representation. While prior work has injected such potential and even proposed training-free methods, there remain key gaps. Consequently, we propose a novel training-free HOI detection framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively utilizes textual and visual interaction representations within a multimodal registry, enabling robust and nuanced interaction understanding. This registry incorporates a small set of visual cues and uses innovative interaction signatures to improve the semantic alignment of verbs, facilitating effective generalization to rare interactions. Additionally, we propose a unique multi-head attention mechanism that adaptively weights the contributions of the visual and textual features. Experimental results demonstrate that our DYSCO surpasses training-free state-of-the-art models and is competitive with training-based approaches, particularly excelling in rare interactions. Code is available at this https URL.
#### Summary:
这篇论文提出了一种名为DYSCO的新型无训练人类-物体交互（HOI）检测框架，利用视觉-语言模型（VLMs）增强交互表示。该方法通过多模态注册表结合文本和视觉交互表示，采用创新的交互签名和多头注意力机制，提高了语义对齐和罕见交互的泛化能力。实验表明，DYSCO在无训练方法中表现优异，并与基于训练的方法竞争。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17456

### 23. An h-space Based Adversarial Attack for Protection Against Few-shot Personalization
#### Abstract:
The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness.
#### Summary:
这篇论文提出了一种基于h空间的对抗攻击方法（HAAD），用于保护隐私内容免受扩散模型的少样本个性化定制。作者观察到扩散模型在语义潜在空间（h-space）中表现出高度抽象性，因此设计了HAAD和其高效变体HAAD-KV，通过扰动h空间来破坏图像生成过程。实验表明，该方法在计算效率和保护强度上优于现有对抗攻击方法。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17554

### 24. Yume: An Interactive World Generation Model
#### Abstract:
Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on this https URL. Yume will update monthly to achieve its original goal. Project page: this https URL.
#### Summary:
Yume是一个交互式世界生成模型，旨在通过图像、文本或视频创建动态、逼真的交互世界，支持通过外设或神经信号进行探索和控制。论文提出了一个预览版本，包含相机运动量化、视频生成架构、高级采样器和模型加速四个主要组件，使用高质量数据集进行训练，并在多样场景中取得显著效果。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17744

### 25. BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems
#### Abstract:
Large language models (LLMs) are growingly extended to process multimodal data such as text and video simultaneously. Their remarkable performance in understanding what is shown in images is surpassing specialized neural networks (NNs) such as Yolo that is supporting only a well-formed but very limited vocabulary, ie., objects that they are able to detect. When being non-restricted, LLMs and in particular state-of-the-art vision language models (VLMs) show impressive performance to describe even complex traffic situations. This is making them potentially suitable components for automotive perception systems to support the understanding of complex traffic situations or edge case situation. However, LLMs and VLMs are prone to hallucination, which mean to either potentially not seeing traffic agents such as vulnerable road users who are present in a situation, or to seeing traffic agents who are not there in reality. While the latter is unwanted making an ADAS or autonomous driving systems (ADS) to unnecessarily slow down, the former could lead to disastrous decisions from an ADS. In our work, we are systematically assessing the performance of 3 state-of-the-art VLMs on a diverse subset of traffic situations sampled from the Waymo Open Dataset to support safety guardrails for capturing such hallucinations in VLM-supported perception systems. We observe that both, proprietary and open VLMs exhibit remarkable image understanding capabilities even paying thorough attention to fine details sometimes difficult to spot for us humans. However, they are also still prone to making up elements in their descriptions to date requiring hallucination detection strategies such as BetterCheck that we propose in our work.
#### Summary:
这篇论文探讨了大型语言模型（LLMs）和视觉语言模型（VLMs）在汽车感知系统中的应用，特别是它们在理解复杂交通场景中的潜力。作者指出，尽管VLMs在图像理解方面表现出色，但它们容易产生幻觉（hallucination），即可能忽略实际存在的交通参与者或虚构不存在的对象。为了解决这一问题，论文提出了BetterCheck方法，用于系统地评估三种最先进的VLMs在多样化交通场景中的表现，并提出了幻觉检测策略。研究基于Waymo Open Dataset的数据，发现无论是专有还是开源的VLMs，在图像理解方面都有显著能力，但仍需改进以避免幻觉。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17722

### 26. LoRA is All You Need for Safety Alignment of Reasoning LLMs
#### Abstract:
Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex problems that were previously out of reach. To ensure LLMs do not assist with harmful requests, safety alignment fine-tuning is necessary in the post-training phase. However, safety alignment fine-tuning has recently been shown to significantly degrade reasoning abilities, a phenomenon known as the "Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets effectively aligns the model for safety without harming its reasoning capabilities. This is because restricting the safety weight updates to a low-rank space minimizes the interference with the reasoning weights. Our extensive experiments across four benchmarks covering math, science, and coding show that this approach produces highly safe LLMs -- with safety levels comparable to full-model fine-tuning -- without compromising their reasoning abilities. Additionally, we observe that LoRA induces weight updates with smaller overlap with the initial weights compared to full-model fine-tuning. We also explore methods that further reduce such overlap -- via regularization or during weight merging -- and observe some improvement on certain tasks. We hope this result motivates designing approaches that yield more consistent improvements in the reasoning-safety trade-off.
#### Summary:
这篇论文探讨了在大型语言模型（LLMs）的安全对齐微调过程中，使用LoRA（低秩适应）技术来避免对模型推理能力的负面影响。研究发现，通过在拒绝数据集上进行LoRA微调，可以有效提升模型的安全性，同时不损害其推理能力。这是因为LoRA将安全权重更新限制在低秩空间，减少了对推理权重的干扰。实验表明，这种方法在数学、科学和编程等多个基准测试中均能保持高安全性和良好的推理能力。此外，研究还发现LoRA引起的权重更新与初始权重的重叠较小，并探索了进一步减少这种重叠的方法。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17075

### 27. Each to Their Own: Exploring the Optimal Embedding in RAG
#### Abstract:
Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.
#### Summary:
这篇论文探讨了在检索增强生成（RAG）中如何优化嵌入模型的选择。作者指出，由于不同嵌入模型的异构性和训练数据的差异，它们在相似性计算和响应质量上表现不一。为此，他们提出了两种方法：Mixture-Embedding RAG和Confident RAG。前者通过标准化相似性排序选择检索结果，但效果不如传统RAG；后者则通过多次生成响应并选择置信度最高的结果，平均性能提升了约10%和5%。该方法在不同LLM和嵌入模型中表现一致，是一种高效的即插即用方案。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17442

### 28. Towards Trustworthy AI: Secure Deepfake Detection using CNNs and Zero-Knowledge Proofs
#### Abstract:
In the era of synthetic media, deepfake manipulations pose a significant threat to information integrity. To address this challenge, we propose TrustDefender, a two-stage framework comprising (i) a lightweight convolutional neural network (CNN) that detects deepfake imagery in real-time extended reality (XR) streams, and (ii) an integrated succinct zero-knowledge proof (ZKP) protocol that validates detection results without disclosing raw user data. Our design addresses both the computational constraints of XR platforms while adhering to the stringent privacy requirements in sensitive settings. Experimental evaluations on multiple benchmark deepfake datasets demonstrate that TrustDefender achieves 95.3% detection accuracy, coupled with efficient proof generation underpinned by rigorous cryptography, ensuring seamless integration with high-performance artificial intelligence (AI) systems. By fusing advanced computer vision models with provable security mechanisms, our work establishes a foundation for reliable AI in immersive and privacy-sensitive applications.
#### Summary:
这篇论文提出了TrustDefender，一个两阶段的框架，用于检测深度伪造（deepfake）图像。第一阶段使用轻量级卷积神经网络（CNN）实时检测扩展现实（XR）流中的深度伪造图像；第二阶段集成简洁的零知识证明（ZKP）协议，在不泄露原始用户数据的情况下验证检测结果。该设计既考虑了XR平台的计算限制，又满足了敏感场景下的严格隐私要求。实验表明，TrustDefender在多个基准数据集上达到了95.3%的检测准确率，并结合了严格的密码学支持，确保与高性能人工智能系统的无缝集成。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17010

### 29. SplitMeanFlow: Interval Splitting Consistency in Few-Step Generative Modeling
#### Abstract:
Generative models like Flow Matching have achieved state-of-the-art performance but are often hindered by a computationally expensive iterative sampling process. To address this, recent work has focused on few-step or one-step generation by learning the average velocity field, which directly maps noise to data. MeanFlow, a leading method in this area, learns this field by enforcing a differential identity that connects the average and instantaneous velocities. In this work, we argue that this differential formulation is a limiting special case of a more fundamental principle. We return to the first principles of average velocity and leverage the additivity property of definite integrals. This leads us to derive a novel, purely algebraic identity we term Interval Splitting Consistency. This identity establishes a self-referential relationship for the average velocity field across different time intervals without resorting to any differential operators. Based on this principle, we introduce SplitMeanFlow, a new training framework that enforces this algebraic consistency directly as a learning objective. We formally prove that the differential identity at the core of MeanFlow is recovered by taking the limit of our algebraic consistency as the interval split becomes infinitesimal. This establishes SplitMeanFlow as a direct and more general foundation for learning average velocity fields. From a practical standpoint, our algebraic approach is significantly more efficient, as it eliminates the need for JVP computations, resulting in simpler implementation, more stable training, and broader hardware compatibility. One-step and two-step SplitMeanFlow models have been successfully deployed in large-scale speech synthesis products (such as Doubao), achieving speedups of 20x.
#### Summary:
这篇论文提出了一种名为SplitMeanFlow的新方法，用于改进生成模型中的少步或一步生成过程。传统Flow Matching方法依赖于计算密集的迭代采样，而MeanFlow通过学习平均速度场直接映射噪声到数据。作者从积分的基本原理出发，提出了一种称为区间分割一致性（Interval Splitting Consistency）的代数恒等式，取代了原有的微分恒等式。SplitMeanFlow通过直接优化这一代数一致性目标，避免了复杂的JVP计算，实现了更高效、稳定的训练，并在大规模语音合成产品中取得了20倍的加速效果。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.16884

### 30. SADA: Stability-guided Adaptive Diffusion Acceleration
#### Abstract:
Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim 0.01$ spectrogram LPIPS.
#### Summary:
这篇论文提出了一种名为SADA（Stability-guided Adaptive Diffusion Acceleration）的新方法，旨在加速基于ODE的生成模型（如扩散模型和流匹配模型）的采样过程。SADA通过稳定性准则统一了步级和令牌级的稀疏决策，自适应地分配稀疏性，并利用ODE求解器的精确梯度信息进行近似。实验表明，SADA在SD-2、SDXL和Flux等模型上实现了≥1.8倍的加速，且保真度损失最小（LPIPS≤0.10，FID≤4.5），显著优于现有方法。此外，SADA还能无缝适应其他管道和模态，如ControlNet和MusicLDM。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17135

### 31. DistrAttention: An Efficient and Flexible Self-Attention Mechanism on Modern GPUs
#### Abstract:
The Transformer architecture has revolutionized deep learning, delivering the state-of-the-art performance in areas such as natural language processing, computer vision, and time series prediction. However, its core component, self-attention, has the quadratic time complexity relative to input sequence length, which hinders the scalability of Transformers. The exsiting approaches on optimizing self-attention either discard full-contextual information or lack of flexibility. In this work, we design DistrAttention, an effcient and flexible self-attention mechanism with the full context. DistrAttention achieves this by grouping data on the embedding dimensionality, usually referred to as $d$. We realize DistrAttention with a lightweight sampling and fusion method that exploits locality-sensitive hashing to group similar data. A block-wise grouping framework is further designed to limit the errors introduced by locality sensitive hashing. By optimizing the selection of block sizes, DistrAttention could be easily integrated with FlashAttention-2, gaining high-performance on modern GPUs. We evaluate DistrAttention with extensive experiments. The results show that our method is 37% faster than FlashAttention-2 on calculating self-attention. In ViT inference, DistrAttention is the fastest and the most accurate among approximate self-attention mechanisms. In Llama3-1B, DistrAttention still achieves the lowest inference time with only 1% accuray loss.
#### Summary:
这篇论文提出了一种名为DistrAttention的高效且灵活的自注意力机制，旨在解决Transformer架构中自注意力计算复杂度高的问题。DistrAttention通过在嵌入维度上分组数据，并利用局部敏感哈希和块状分组框架来优化计算，同时保持完整的上下文信息。实验表明，DistrAttention在计算自注意力时比FlashAttention-2快37%，在ViT推理中表现最佳，并在Llama3-1B中实现了最低的推理时间，仅损失1%的准确率。
#### Relevance Score: 7.0
#### PDF URL: https://arxiv.org/pdf/2507.17245

### 32. AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation
#### Abstract:
Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm shift from static prediction systems to agentic AI agents capable of reasoning, interacting with tools, and adapting to complex tasks. While LLM-based agentic systems have shown promise across many domains, their application to medical imaging remains in its infancy. In this work, we introduce AURA, the first visual linguistic explainability agent designed specifically for comprehensive analysis, explanation, and evaluation of medical images. By enabling dynamic interactions, contextual explanations, and hypothesis testing, AURA represents a significant advancement toward more transparent, adaptable, and clinically aligned AI systems. We highlight the promise of agentic AI in transforming medical image analysis from static predictions to interactive decision support. Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular toolbox comprising: (i) a segmentation suite with phase grounding, pathology segmentation, and anatomy segmentation to localize clinically meaningful regions; (ii) a counterfactual image-generation module that supports reasoning through image-level explanations; and (iii) a set of evaluation tools including pixel-wise difference-map analysis, classification, and advanced state-of-the-art components to assess diagnostic relevance and visual interpretability.
#### Summary:
这篇论文介绍了AURA，一个多模态医疗代理，专为医学图像的全面分析、解释和评估而设计。AURA利用大型语言模型（LLMs）和模块化工具箱，包括分割套件、反事实图像生成模块和评估工具，旨在实现动态交互、上下文解释和假设测试，从而提升医学图像分析的透明度和临床适用性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.16940

### 33. Vec2Face+ for Face Dataset Generation
#### Abstract:
When synthesizing identities as face recognition training data, it is generally believed that large inter-class separability and intra-class attribute variation are essential for synthesizing a quality dataset. % This belief is generally correct, and this is what we aim for. However, when increasing intra-class variation, existing methods overlook the necessity of maintaining intra-class identity consistency. % To address this and generate high-quality face training data, we propose Vec2Face+, a generative model that creates images directly from image features and allows for continuous and easy control of face identities and attributes. Using Vec2Face+, we obtain datasets with proper inter-class separability and intra-class variation and identity consistency using three strategies: 1) we sample vectors sufficiently different from others to generate well-separated identities; 2) we propose an AttrOP algorithm for increasing general attribute variations; 3) we propose LoRA-based pose control for generating images with profile head poses, which is more efficient and identity-preserving than AttrOP. % Our system generates VFace10K, a synthetic face dataset with 10K identities, which allows an FR model to achieve state-of-the-art accuracy on seven real-world test sets. Scaling the size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets yield higher accuracy than the real-world training dataset, CASIA-WebFace, on five real-world test sets. This is the first time a synthetic dataset beats the CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11 synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin verification and that models trained with synthetic identities are more biased than those trained with real identities. Both are important aspects for future investigation.
#### Summary:
这篇论文介绍了Vec2Face+，一种用于生成人脸识别训练数据的生成模型。该模型通过直接从图像特征创建图像，并允许连续和轻松地控制人脸身份和属性，从而生成具有适当类间分离性和类内变化的数据集。论文提出了三种策略来确保身份一致性和属性变化，并生成了VFace10K、VFace100K和VFace300K等合成数据集。实验表明，这些数据集在多个真实世界测试集上达到了最先进的准确率，甚至超过了真实数据集CASIA-WebFace。此外，论文还发现合成数据集在双胞胎验证和模型偏差方面存在一些问题，值得未来研究。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17192

### 34. CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits
#### Abstract:
With the rapid advancement of large foundation models, AIGC, cloud rendering, and real-time motion capture technologies, digital humans are now capable of achieving synchronized facial expressions and body movements, engaging in intelligent dialogues driven by natural language, and enabling the fast creation of personalized avatars. While current mainstream approaches to digital humans primarily focus on 3D models and 2D video-based representations, interactive 2D cartoon-style digital humans have received relatively less attention. Compared to 3D digital humans that require complex modeling and high rendering costs, and 2D video-based solutions that lack flexibility and real-time interactivity, 2D cartoon-style Live2D models offer a more efficient and expressive alternative. By simulating 3D-like motion through layered segmentation without the need for traditional 3D modeling, Live2D enables dynamic and real-time manipulation. In this technical report, we present CartoonAlive, an innovative method for generating high-quality Live2D digital humans from a single input portrait image. CartoonAlive leverages the shape basis concept commonly used in 3D face modeling to construct facial blendshapes suitable for Live2D. It then infers the corresponding blendshape weights based on facial keypoints detected from the input image. This approach allows for the rapid generation of a highly expressive and visually accurate Live2D model that closely resembles the input portrait, within less than half a minute. Our work provides a practical and scalable solution for creating interactive 2D cartoon characters, opening new possibilities in digital content creation and virtual character animation. The project homepage is this https URL.
#### Summary:
这篇论文介绍了CartoonAlive，一种从单张肖像图像生成高质量Live2D数字人的创新方法。Live2D是一种2D卡通风格的数字人建模技术，通过分层分割模拟3D动作，无需传统3D建模，实现动态实时操作。CartoonAlive利用3D面部建模中常用的形状基概念，构建适合Live2D的面部混合形状，并根据输入图像检测到的面部关键点推断相应的混合形状权重。该方法能在不到半分钟内生成与输入肖像高度相似、表现力强的Live2D模型，为交互式2D卡通角色创建提供了实用且可扩展的解决方案。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17327

### 35. Accelerating Parallel Diffusion Model Serving with Residual Compression
#### Abstract:
Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at this https URL
#### Summary:
这篇论文提出了CompactFusion，一个用于加速并行扩散模型服务的压缩框架。通过观察扩散模型激活的时间冗余性，该框架采用残差压缩技术，仅传输激活的差异部分，从而显著减少通信开销，同时保持生成质量。实验表明，CompactFusion在多个并行设置下实现了显著的加速和更高的生成质量。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17511

### 36. Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation
#### Abstract:
Automated X-ray inspection is crucial for efficient and unobtrusive security screening in various public settings. However, challenges such as object occlusion, variations in the physical properties of items, diversity in X-ray scanning devices, and limited training data hinder accurate and reliable detection of illicit items. Despite the large body of research in the field, reported experimental evaluations are often incomplete, with frequently conflicting outcomes. To shed light on the research landscape and facilitate further research, a systematic, detailed, and thorough comparative evaluation of recent Deep Learning (DL)-based methods for X-ray object detection is conducted. For this, a comprehensive evaluation framework is developed, composed of: a) Six recent, large-scale, and widely used public datasets for X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and PIDray), b) Ten different state-of-the-art object detection schemes covering all main categories in the literature, including generic Convolutional Neural Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer architectures, and c) Various detection (mAP50 and mAP50:95) and time/computational-complexity (inference time (ms), parameter size (M), and computational load (GFLOPS)) metrics. A thorough analysis of the results leads to critical observations and insights, emphasizing key aspects such as: a) Overall behavior of the object detection schemes, b) Object-level detection performance, c) Dataset-specific observations, and d) Time efficiency and computational complexity analysis. To support reproducibility of the reported experimental results, the evaluation code and model weights are made publicly available at this https URL.
#### Summary:
这篇论文对基于深度学习的X射线图像中违禁物品检测方法进行了系统的比较评估。作者开发了一个全面的评估框架，包括六个大型公开数据集、十种最先进的物体检测方案（涵盖CNN、Transformer及混合架构），以及多种检测和计算复杂度指标。通过详细分析，论文揭示了不同方法在性能、数据集适应性和计算效率方面的差异，并公开了代码和模型权重以促进可重复性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17508

### 37. URPO: A Unified Reward & Policy Optimization Framework for Large Language Models
#### Abstract:
Large-scale alignment pipelines typically pair a policy model with a separately trained reward model whose parameters remain frozen during reinforcement learning (RL). This separation creates a complex, resource-intensive pipeline and suffers from a performance ceiling due to a static reward signal. We propose a novel framework, Unified Reward & Policy Optimization (URPO), that unifies instruction-following ("player") and reward modeling ("referee") within a single model and a single training phase. Our method recasts all alignment data-including preference pairs, verifiable reasoning, and open-ended instructions-into a unified generative format optimized by a single Group-Relative Policy Optimization (GRPO) loop. This enables the model to learn from ground-truth preferences and verifiable logic while simultaneously generating its own rewards for open-ended tasks. Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified model significantly outperforms a strong baseline using a separate generative reward model, boosting the instruction-following score on AlpacaEval from 42.24 to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore, URPO cultivates a superior internal evaluator as a byproduct of training, achieving a RewardBench score of 85.15 and surpassing the dedicated reward model it replaces (83.55). By eliminating the need for a separate reward model and fostering a co-evolutionary dynamic between generation and evaluation, URPO presents a simpler, more efficient, and more effective path towards robustly aligned language models.
#### Summary:
这篇论文提出了一个名为URPO（Unified Reward & Policy Optimization）的新框架，旨在统一大型语言模型中的指令遵循（“玩家”）和奖励建模（“裁判”）功能。通过将所有的对齐数据（包括偏好对、可验证推理和开放式指令）转换为统一的生成格式，并使用单一的Group-Relative Policy Optimization（GRPO）循环进行优化，URPO能够在单个模型和单个训练阶段中同时学习地面真实偏好和生成自己的奖励。实验表明，URPO在Qwen2.5-7B模型上表现优异，显著提升了指令遵循和复合推理的平均得分，并且其内部评估器也优于独立的奖励模型。URPO简化了传统的大规模对齐流程，提供了一种更高效、更有效的语言模型对齐方法。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17515

### 38. Vision Transformer attention alignment with human visual perception in aesthetic object evaluation
#### Abstract:
Visual attention mechanisms play a crucial role in human perception and aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have demonstrated remarkable capabilities in computer vision tasks, yet their alignment with human visual attention patterns remains underexplored, particularly in aesthetic contexts. This study investigates the correlation between human visual attention and ViT attention mechanisms when evaluating handcrafted objects. We conducted an eye-tracking experiment with 30 participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal objects comprising basketry bags and ginger jars. Using a Pupil Labs eye-tracker, we recorded gaze patterns and generated heat maps representing human visual attention. Simultaneously, we analyzed the same objects using a pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting attention maps from each of the 12 attention heads. We compared human and ViT attention distributions using Kullback-Leibler divergence across varying Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest alignment with human visual patterns. Significant differences were found between attention heads, with heads #7 and #9 demonstrating the greatest divergence from human attention (p< 0.05, Tukey HSD test). Results indicate that while ViTs exhibit more global attention patterns compared to human focal attention, certain attention heads can approximate human visual behavior, particularly for specific object features like buckles in basketry items. These findings suggest potential applications of ViT attention mechanisms in product design and aesthetic evaluation, while highlighting fundamental differences in attention strategies between human perception and current AI models.
#### Summary:
这篇论文研究了视觉变换器（ViT）的注意力机制与人类视觉注意力在美学对象评估中的对齐情况。通过眼动追踪实验和预训练的ViT模型（使用DINO方法），研究人员比较了人类和ViT在观看手工艺品时的注意力分布。研究发现，某些ViT注意力头（如第12头）与人类视觉模式有较强的相关性，而其他头（如第7和第9头）则表现出较大差异。这表明ViT在美学评估中可能具有应用潜力，但也揭示了人类与AI在注意力策略上的根本差异。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17616

### 39. Attention (as Discrete-Time Markov) Chains
#### Abstract:
We introduce a new interpretation of the attention matrix as a discrete-time Markov chain. Our interpretation sheds light on common operations involving attention scores such as selection, summation, and averaging in a unified framework. It further extends them by considering indirect attention, propagated through the Markov chain, as opposed to previous studies that only model immediate effects. Our main observation is that tokens corresponding to semantically similar regions form a set of metastable states, where the attention clusters, while noisy attention scores tend to disperse. Metastable states and their prevalence can be easily computed through simple matrix multiplication and eigenanalysis, respectively. Using these lightweight tools, we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define TokenRank -- the steady state vector of the Markov chain, which measures global token importance. We demonstrate that using it brings improvements in unconditional image generation. We believe our framework offers a fresh view of how tokens are being attended in modern visual transformers.
#### Summary:
这篇论文提出了一种新的解释，将注意力矩阵视为离散时间马尔可夫链。该解释在一个统一的框架下阐明了涉及注意力分数的常见操作，如选择、求和和平均，并通过考虑通过马尔可夫链传播的间接注意力扩展了这些操作。作者的主要观察是，对应于语义相似区域的标记形成了一组亚稳态，其中注意力聚集，而噪声注意力分数则倾向于分散。亚稳态及其普遍性可以通过简单的矩阵乘法和特征分析轻松计算。作者展示了在零样本分割中的最先进性能，并定义了TokenRank（马尔可夫链的稳态向量）来衡量全局标记重要性，在无条件图像生成中带来了改进。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17657

### 40. Towards Autonomous Sustainability Assessment via Multimodal AI Agents
#### Abstract:
Interest in sustainability information has surged in recent years. However, the data required for a life cycle assessment (LCA) that maps the materials and processes from product manufacturing to disposal into environmental impacts (EI) are often unavailable. Here we reimagine conventional LCA by introducing multimodal AI agents that emulate interactions between LCA experts and stakeholders like product managers and engineers to calculate the cradle-to-gate (production) carbon emissions of electronic devices. The AI agents iteratively generate a detailed life-cycle inventory leveraging a custom data abstraction and software tools that extract information from online text and images from repair communities and government certifications. This approach reduces weeks or months of expert time to under one minute and closes data availability gaps while yielding carbon footprint estimates within 19% of expert LCAs with zero proprietary data. Additionally, we develop a method to directly estimate EI by comparing an input to a cluster of products with similar descriptions and known carbon footprints. This runs in 3 ms on a laptop with a MAPE of 12.28% on electronic products. Further, we develop a data-driven method to generate emission factors. We use the properties of an unknown material to represent it as a weighted sum of emission factors for similar materials. Compared to human experts picking the closest LCA database entry, this improves MAPE by 120.26%. We analyze the data and compute scaling of this approach and discuss its implications for future LCA workflows.
#### Summary:
这篇论文提出了一种利用多模态AI代理重新构想传统生命周期评估（LCA）的方法，通过模拟LCA专家与利益相关者之间的互动，快速计算电子设备从生产到报废的碳排放。该方法利用自定义数据抽象和软件工具从在线文本和图像中提取信息，显著减少了专家所需时间，并填补了数据可用性空白。此外，论文还开发了直接估计环境影响的方法和数据驱动的排放因子生成方法，提高了准确性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17012

### 41. Improving LLMs' Generalized Reasoning Abilities by Graph Problems
#### Abstract:
Large Language Models (LLMs) have made remarkable strides in reasoning tasks, yet their performance often falters on novel and complex problems. Domain-specific continued pretraining (CPT) methods, such as those tailored for mathematical reasoning, have shown promise but lack transferability to broader reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning (GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks, spanning pathfinding, network analysis, numerical computation, and topological reasoning, require sophisticated logical and relational reasoning, making them ideal for teaching diverse reasoning patterns. To achieve this, we introduce GraphPile, the first large-scale corpus specifically designed for CPT using GPR data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes chain-of-thought, program-of-thought, trace of execution, and real-world graph data. Using GraphPile, we train GraphMind on popular base models Llama 3 and 3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in mathematical reasoning and up to 21.2 percent improvement in non-mathematical reasoning tasks such as logical and commonsense reasoning. By being the first to harness GPR for enhancing reasoning patterns and introducing the first dataset of its kind, our work bridges the gap between domain-specific pretraining and universal reasoning capabilities, advancing the adaptability and robustness of LLMs.
#### Summary:
这篇论文提出了一种通过图问题推理（GPR）来增强大型语言模型（LLMs）通用推理能力的方法。作者引入了GraphPile，这是一个专门为GPR设计的大规模语料库，包含10.9亿个标记和23个图任务。通过在Llama 3、3.1和Gemma 2等基础模型上训练GraphMind，论文展示了在数学推理和非数学推理任务（如逻辑和常识推理）上的显著性能提升。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17168

### 42. An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models
#### Abstract:
Large Language Models (LLMs) have demonstrated remarkable progress in instruction following and general-purpose reasoning. However, achieving high-quality alignment with human intent and safety norms without human annotations remains a fundamental challenge. In this work, we propose an Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to improve LLM alignment in a fully automated manner. UDASA first generates multiple responses for each input and quantifies output uncertainty across three dimensions: semantics, factuality, and value alignment. Based on these uncertainty scores, the framework constructs preference pairs and categorizes training samples into three stages, conservative, moderate, and exploratory, according to their uncertainty difference. The model is then optimized progressively across these stages. In addition, we conduct a series of preliminary studies to validate the core design assumptions and provide strong empirical motivation for the proposed framework. Experimental results show that UDASA outperforms existing alignment methods across multiple tasks, including harmlessness, helpfulness, truthfulness, and controlled sentiment generation, significantly improving model performance.
#### Summary:
这篇论文提出了一个名为UDASA（Uncertainty-Driven Adaptive Self-Alignment）的框架，旨在通过完全自动化的方式改进大型语言模型（LLMs）的对齐质量。UDASA通过生成多个响应并量化输出在语义、事实性和价值对齐三个维度上的不确定性，构建偏好对并将训练样本分为保守、中等和探索三个阶段，逐步优化模型。实验结果表明，UDASA在无害性、帮助性、真实性和受控情感生成等多个任务上优于现有对齐方法。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17477

### 43. Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs
#### Abstract:
Conversational Information Retrieval (CIR) systems, while offering intuitive access to information, face a significant challenge: reliably handling unanswerable questions to prevent the generation of misleading or hallucinated content. Traditional approaches often rely on external classifiers, which can introduce inconsistencies with the core generative Large Language Models (LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a novel approach that deeply integrates unanswerability detection directly within the LLM's generative process. SALU is trained using a multi-task learning framework for both standard Question Answering (QA) and explicit abstention generation for unanswerable queries. Crucially, it incorporates a confidence-score-guided reinforcement learning with human feedback (RLHF) phase, which explicitly penalizes hallucinated responses and rewards appropriate abstentions, fostering intrinsic self-awareness of knowledge boundaries. Through extensive experiments on our custom-built C-IR_Answerability dataset, SALU consistently outperforms strong baselines, including hybrid LLM-classifier systems, in overall accuracy for correctly answering or abstaining from questions. Human evaluation further confirms SALU's superior reliability, achieving high scores in factuality, appropriate abstention, and, most importantly, a dramatic reduction in hallucination, demonstrating its ability to robustly "know when to say 'I don't know'."
#### Summary:
这篇论文提出了一种名为SALU的新方法，旨在通过深度整合不可回答性检测到大型语言模型（LLMs）的生成过程中，解决对话信息检索（CIR）系统中处理不可回答问题时的挑战。SALU采用多任务学习框架，结合了标准问答（QA）和显式放弃生成，并通过基于置信度分数的强化学习与人类反馈（RLHF）阶段，显著减少了幻觉内容的生成，提高了系统的可靠性和自我认知能力。实验结果表明，SALU在自定义数据集上表现优于现有基线系统，并在人类评估中显示出高事实性和适当的放弃能力。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.16951

### 44. Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain
#### Abstract:
Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Although large language models (LLMs) can be used to implement Question Answering (QA) systems, simply using publicly available general-purpose LLMs in agriculture typically offer generic advisories, lacking precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets. Our study addresses these limitations by generating multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tuning language-specific LLMs. Our evaluation on curated multilingual datasets demonstrates significant improvements in factual accuracy, relevance, and agricultural consensus for the fine-tuned models compared to their baseline counterparts. These results highlight the efficacy of synthetic data-driven, language-specific fine-tuning as an effective strategy to improve the performance of LLMs in agriculture, especially in multilingual and low-resource settings. By enabling more accurate and localized agricultural advisory services, this study provides a meaningful step toward bridging the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.
#### Summary:
这篇论文研究了如何利用合成数据和多语言大语言模型（LLMs）在农业领域实现多语言问答系统。作者通过从农业相关文档生成多语言合成数据集（英语、印地语、旁遮普语），并对特定语言的LLMs进行微调，显著提高了模型在事实准确性、相关性和农业共识方面的表现。研究结果表明，合成数据驱动的语言特定微调是提升LLMs在农业领域性能的有效策略，尤其是在多语言和低资源环境下。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.16974

### 45. Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?
#### Abstract:
Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at this https URL.
#### Summary:
这篇论文探讨了如何通过外部验证工具提高大型语言模型（LLMs）作为评判者的注释质量。作者提出了一个使用工具的系统，通过网页搜索和代码执行来验证模型响应的准确性，从而在长事实、数学和代码任务等挑战性领域提供更高质量的反馈。实验结果表明，外部工具在许多情况下可以提高性能，但也强调了性能对简单参数（如提示）的敏感性以及改进注释基准的必要性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17015

### 46. CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards
#### Abstract:
Role-Playing Language Agents (RPLAs) have emerged as a significant application direction for Large Language Models (LLMs). Existing approaches typically rely on prompt engineering or supervised fine-tuning to enable models to imitate character behaviors in specific scenarios, but often neglect the underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a \textit{cognize-then-respond } reasoning paradigm. By jointly modeling external situational awareness and internal self-awareness, CogDual generates responses with improved character consistency and contextual alignment. To further optimize the performance, we employ reinforcement learning with two general-purpose reward schemes designed for open-domain text generation. Extensive experiments on the CoSER benchmark, as well as Cross-MR and LifeChoice, demonstrate that CogDual consistently outperforms existing baselines and generalizes effectively across diverse role-playing tasks.
#### Summary:
这篇论文提出了CogDual，一种通过强化学习增强大型语言模型（LLMs）双重认知能力的新方法。CogDual采用“认知-响应”推理范式，结合外部情境意识和内部自我意识，以提升角色扮演语言代理（RPLAs）的角色一致性和上下文对齐。论文还设计了两种通用奖励方案，用于开放域文本生成的强化学习优化。实验表明，CogDual在多个基准测试中表现优于现有基线，并能有效泛化到多样化的角色扮演任务中。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17147

### 47. Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors
#### Abstract:
Suicide remains a pressing global health crisis, with over 720,000 deaths annually and millions more affected by suicide ideation (SI) and suicide attempts (SA). Early identification of suicidality-related factors (SrFs), including SI, SA, exposure to suicide (ES), and non-suicidal self-injury (NSSI), is critical for timely intervention. While prior studies have applied AI to detect SrFs in clinical notes, most treat suicidality as a binary classification task, overlooking the complexity of cooccurring risk factors. This study explores the use of generative large language models (LLMs), specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs from psychiatric electronic health records (EHRs). We present a novel end to end generative MLC pipeline and introduce advanced evaluation methods, including label set level metrics and a multilabel confusion matrix for error analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior performance across label sets, including rare or minority label sets, indicating a more balanced and robust performance. Our findings reveal systematic error patterns, such as the conflation of SI and SA, and highlight the models tendency toward cautious over labeling. This work not only demonstrates the feasibility of using generative AI for complex clinical classification tasks but also provides a blueprint for structuring unstructured EHR data to support large scale clinical research and evidence based medicine.
#### Summary:
这篇论文探讨了在医疗健康领域使用生成式大型语言模型（如GPT-3.5和GPT-4.5）进行多标签分类（MLC）的应用，特别是针对自杀倾向相关因素（SrFs）的分类。研究提出了一种新颖的端到端生成式MLC流程，并引入了先进的评估方法，包括标签集级别的指标和多标签混淆矩阵用于错误分析。研究发现，经过微调的GPT-3.5在部分匹配准确率和F1分数上表现最佳，而GPT-4.5在引导提示下表现出更平衡和鲁棒的性能。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17009

### 48. SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs
#### Abstract:
Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at this https URL.
#### Summary:
这篇论文提出了一个名为SKA-Bench的细粒度基准测试，用于评估大型语言模型（LLMs）在结构化知识（如知识图谱和表格）理解方面的能力。该基准涵盖了四种广泛使用的结构化知识形式，并通过三阶段流程构建测试实例，包括问题、答案、正面知识单元和噪声知识单元。论文还扩展了四个基本能力测试集：噪声鲁棒性、顺序不敏感性、信息整合和负面拒绝。通过对8种代表性LLMs的实证评估，发现现有模型在理解结构化知识方面仍面临挑战，且性能受噪声量、知识单元顺序和幻觉现象等因素影响。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17178

### 49. The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models
#### Abstract:
People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.
#### Summary:
这篇论文研究了大型语言模型（LLMs）与人类在道德判断上的差异。作者通过引入道德困境数据集，比较了LLMs和人类在道德判断分布上的差异，发现LLMs在高共识情况下能复现人类判断，但在分歧增加时对齐性下降。此外，LLMs依赖的道德价值观比人类更狭窄。为了缩小这一差距，作者提出了动态道德分析（DMP）方法，通过基于人类价值分布的采样方法，提高了对齐性和价值多样性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17216

### 50. WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training
#### Abstract:
Recent advances in learning rate (LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies-including cosine decay, linear decay and inverse square root decay-as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration-the training window for checkpoint aggregation-as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.
#### Summary:
这篇论文提出了WSM（Warmup-Stable and Merge）框架，通过模型合并技术模拟学习率衰减策略，避免了传统学习率衰减阶段。WSM将学习率衰减与模型合并建立理论联系，支持多种衰减策略（如余弦衰减、线性衰减等）作为模型平均方案。实验表明，合并持续时间是影响模型性能的关键因素，WSM在多个基准测试中优于广泛采用的WSD方法，并在监督微调场景中展现出长期模型优化的潜力。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17634

### 51. Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models
#### Abstract:
Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, we introduce Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. We conduct a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. Our findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. We integrate these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate our derived scaling laws, we designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.
#### Summary:
这篇论文研究了混合专家（Mixture-of-Experts, MoE）语言模型的高效扩展问题，提出了效率杠杆（Efficiency Leverage, EL）这一指标来量化MoE模型相对于密集模型的优势。通过大规模实验，论文发现EL主要由专家激活比例和总计算预算决定，并遵循可预测的幂律关系，而专家粒度则作为非线性调节器。论文还提出了一个统一的扩展定律，用于预测MoE架构的EL，并通过实验验证了其准确性。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17702

### 52. A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)
#### Abstract:
Large language models are computationally expensive due to their deep structures. Prior research has shown that intermediate layers contain sufficient information to generate accurate answers, leading to the development of early-exit algorithms that reduce inference costs by terminating computation at earlier layers. However, these methods often suffer from poor performance due to misalignment between intermediate and output layer representations that lead to decoding inaccuracy. To address these challenges, we propose SPADE (SPace Alignment DEcoding), a novel decoding method that aligns intermediate layer representations with the output layer by propagating a minimally reduced sequence consisting of only the start token and the answer token. We further optimize the early-exit decision-making process by training a linear approximation of SPADE that computes entropy-based confidence metrics. Putting them together, we create a hybrid early-exit algorithm that monitors confidence levels and stops inference at intermediate layers while using SPADE to generate high-quality outputs. This approach significantly reduces inference costs without compromising accuracy, offering a scalable and efficient solution for deploying large language models in real-world applications.
#### Summary:
这篇论文提出了一种名为SPADE（SPace Alignment DEcoding）的新型解码方法，旨在解决大型语言模型（LLM）在早期退出算法中因中间层与输出层表示不对齐而导致的解码不准确问题。SPADE通过传播仅包含起始标记和答案标记的最小化序列来对齐中间层表示，并进一步优化早期退出决策过程，通过训练线性近似SPADE来计算基于熵的置信度指标。结合这些方法，论文提出了一种混合早期退出算法，可在不牺牲准确性的情况下显著降低推理成本，为实际应用中部署大型语言模型提供了高效解决方案。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17618

### 53. Threshold-Protected Searchable Sharing: Privacy Preserving Aggregated-ANN Search for Collaborative RAG
#### Abstract:
LLM-powered search services have driven data integration as a significant trend. However, this trend's progress is fundamentally hindered, despite the fact that combining individual knowledge can significantly improve the relevance and quality of responses in specialized queries and make AI more professional at providing services. Two key bottlenecks are private data repositories' locality constraints and the need to maintain compatibility with mainstream search techniques, particularly Hierarchical Navigable Small World (HNSW) indexing for high-dimensional vector spaces. In this work, we develop a secure and privacy-preserving aggregated approximate nearest neighbor search (SP-A$^2$NN) with HNSW compatibility under a threshold-based searchable sharing primitive. A sharable bitgraph structure is constructed and extended to support searches and dynamical insertions over shared data without compromising the underlying graph topology. The approach reduces the complexity of a search from $O(n^2)$ to $O(n)$ compared to naive (undirected) graph-sharing approach when organizing graphs in the identical HNSW manner.
On the theoretical front, we explore a novel security analytical framework that incorporates privacy analysis via reductions. The proposed leakage-guessing proof system is built upon an entirely different interactive game that is independent of existing coin-toss game design. Rather than being purely theoretical, this system is rooted in existing proof systems but goes beyond them to specifically address leakage concerns and standardize leakage analysis -- one of the most critical security challenges with AI's rapid development.
#### Summary:
这篇论文提出了一种名为SP-A$^2$NN的安全且隐私保护的聚合近似最近邻搜索方法，兼容HNSW索引技术。通过构建可共享的bitgraph结构，支持在共享数据上进行搜索和动态插入，同时不损害底层图拓扑。该方法将搜索复杂度从$O(n^2)$降低到$O(n)$。论文还提出了一种新的安全分析框架，通过归约进行隐私分析，并构建了一个泄漏猜测证明系统，专门解决AI快速发展中的泄漏问题。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17199

### 54. TD-Interpreter: Enhancing the Understanding of Timing Diagrams with Visual-Language Learning
#### Abstract:
We introduce TD-Interpreter, a specialized ML tool that assists engineers in understanding complex timing diagrams (TDs), originating from a third party, during their design and verification process. TD-Interpreter is a visual question-answer environment which allows engineers to input a set of TDs and ask design and verification queries regarding these TDs. We implemented TD-Interpreter with multimodal learning by fine-tuning LLaVA, a lightweight 7B Multimodal Large Language Model (MLLM). To address limited training data availability, we developed a synthetic data generation workflow that aligns visual information with its textual interpretation. Our experimental evaluation demonstrates the usefulness of TD-Interpreter which outperformed untuned GPT-4o by a large margin on the evaluated benchmarks.
#### Summary:
这篇论文介绍了TD-Interpreter，一种专门用于帮助工程师理解和验证复杂时序图（TDs）的ML工具。该工具通过微调轻量级多模态大语言模型LLaVA，结合视觉问答环境，实现了对时序图的多模态学习。为了解决训练数据不足的问题，作者开发了一种合成数据生成工作流，将视觉信息与文本解释对齐。实验表明，TD-Interpreter在基准测试中显著优于未经调优的GPT-4o。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.16844

### 55. R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning
#### Abstract:
Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of large language models by encouraging step-by-step intermediate reasoning during inference. While effective, CoT introduces substantial computational overhead due to its reliance on autoregressive decoding over long token sequences. Existing acceleration strategies either reduce sequence length through early stopping or compressive reward designs, or improve decoding speed via speculative decoding with smaller models. However, speculative decoding suffers from limited speedup when the agreement between small and large models is low, and fails to exploit the potential advantages of small models in producing concise intermediate reasoning. In this paper, we present R-Stitch, a token-level, confidence-based hybrid decoding framework that accelerates CoT inference by switching between a small language model (SLM) and a large language model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to generate tokens by default and delegates to the LLM only when the SLM's confidence falls below a threshold. This design avoids full-sequence rollback and selectively invokes the LLM on uncertain steps, preserving both efficiency and answer quality. R-Stitch is model-agnostic, training-free, and compatible with standard decoding pipelines. Experiments on math reasoning benchmarks demonstrate that R-Stitch achieves up to 85\% reduction in inference latency with negligible accuracy drop, highlighting its practical effectiveness in accelerating CoT reasoning.
#### Summary:
这篇论文提出了R-Stitch，一种基于置信度的混合解码框架，旨在加速大型语言模型（LLM）的链式思考（CoT）推理过程。R-Stitch通过在推理轨迹上动态切换小型语言模型（SLM）和大型语言模型（LLM）来减少计算开销，仅在SLM置信度低于阈值时调用LLM。这种方法避免了全序列回滚，并在数学推理基准测试中实现了高达85%的推理延迟减少，同时保持了答案质量。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17307

### 56. Rethinking VAE: From Continuous to Discrete Representations Without Probabilistic Assumptions
#### Abstract:
This paper explores the generative capabilities of Autoencoders (AEs) and establishes connections between Variational Autoencoders (VAEs) and Vector Quantized-Variational Autoencoders (VQ-VAEs) through a reformulated training framework. We demonstrate that AEs exhibit generative potential via latent space interpolation and perturbation, albeit limited by undefined regions in the encoding space. To address this, we propose a new VAE-like training method that introduces clustering centers to enhance data compactness and ensure well-defined latent spaces without relying on traditional KL divergence or reparameterization techniques. Experimental results on MNIST, CelebA, and FashionMNIST datasets show smooth interpolative transitions, though blurriness persists. Extending this approach to multiple learnable vectors, we observe a natural progression toward a VQ-VAE-like model in continuous space. However, when the encoder outputs multiple vectors, the model degenerates into a discrete Autoencoder (VQ-AE), which combines image fragments without learning semantic representations. Our findings highlight the critical role of encoding space compactness and dispersion in generative modeling and provide insights into the intrinsic connections between VAEs and VQ-VAEs, offering a new perspective on their design and limitations.
#### Summary:
本文探讨了自编码器（AEs）的生成能力，并通过重新设计的训练框架建立了变分自编码器（VAEs）和向量量化变分自编码器（VQ-VAEs）之间的联系。作者提出了一种新的类似VAE的训练方法，通过引入聚类中心来增强数据紧凑性，并确保潜在空间的良好定义，而不依赖传统的KL散度或重参数化技术。实验结果表明，该方法在MNIST、CelebA和FashionMNIST数据集上实现了平滑的插值过渡，但仍存在模糊问题。进一步研究发现，当编码器输出多个向量时，模型会退化为离散自编码器（VQ-AE），仅组合图像片段而未学习语义表示。研究强调了编码空间紧凑性和分散性在生成建模中的关键作用，并为VAEs和VQ-VAEs的内在联系提供了新视角。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17255

### 57. A Comprehensive Evaluation on Quantization Techniques for Large Language Models
#### Abstract:
For large language models (LLMs), post-training quantization (PTQ) can significantly reduce memory footprint and computational overhead. Model quantization is a rapidly evolving research field. Though many papers have reported breakthrough performance, they may not conduct experiments on the same ground since one quantization method usually contains multiple components. In addition, analyzing the theoretical connections among existing methods is crucial for in-depth understanding. To bridge these gaps, we conduct an extensive review of state-of-the-art methods and perform comprehensive evaluations on the same ground to ensure fair comparisons. To our knowledge, this fair and extensive investigation remains critically important yet underexplored. To better understand the theoretical connections, we decouple the published quantization methods into two steps: pre-quantization transformation and quantization error mitigation. We define the former as a preprocessing step applied before quantization to reduce the impact of outliers, making the data distribution flatter and more suitable for quantization. Quantization error mitigation involves techniques that offset the errors introduced during quantization, thereby enhancing model performance. We evaluate and analyze the impact of different components of quantization methods. Additionally, we analyze and evaluate the latest MXFP4 data format and its performance. Our experimental results demonstrate that optimized rotation and scaling yield the best performance for pre-quantization transformation, and combining low-rank compensation with GPTQ occasionally outperforms using GPTQ alone for quantization error mitigation. Furthermore, we explore the potential of the latest MXFP4 quantization and reveal that the optimal pre-quantization transformation strategy for INT4 does not generalize well to MXFP4, inspiring further investigation.
#### Summary:
这篇论文对大型语言模型（LLMs）的后训练量化（PTQ）技术进行了全面评估，旨在减少内存占用和计算开销。作者回顾了现有最先进的量化方法，并在相同条件下进行了公平比较，以填补该领域的研究空白。论文将量化方法解耦为预量化变换和量化误差缓解两个步骤，并评估了不同组件的影响。实验结果表明，优化的旋转和缩放策略在预量化变换中表现最佳，而低秩补偿与GPTQ结合在量化误差缓解中偶尔优于单独使用GPTQ。此外，论文还探讨了最新的MXFP4数据格式的性能，发现INT4的最佳预量化变换策略不适用于MXFP4，这为进一步研究提供了启示。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17417

### 58. Generalized Dual Discriminator GANs
#### Abstract:
Dual discriminator generative adversarial networks (D2 GANs) were introduced to mitigate the problem of mode collapse in generative adversarial networks. In D2 GANs, two discriminators are employed alongside a generator: one discriminator rewards high scores for samples from the true data distribution, while the other favors samples from the generator. In this work, we first introduce dual discriminator $\alpha$-GANs (D2 $\alpha$-GANs), which combines the strengths of dual discriminators with the flexibility of a tunable loss function, $\alpha$-loss. We further generalize this approach to arbitrary functions defined on positive reals, leading to a broader class of models we refer to as generalized dual discriminator generative adversarial networks. For each of these proposed models, we provide theoretical analysis and show that the associated min-max optimization reduces to the minimization of a linear combination of an $f$-divergence and a reverse $f$-divergence. This generalizes the known simplification for D2-GANs, where the objective reduces to a linear combination of the KL-divergence and the reverse KL-divergence. Finally, we perform experiments on 2D synthetic data and use multiple performance metrics to capture various advantages of our GANs.
#### Summary:
这篇论文提出了广义双判别器生成对抗网络（Generalized Dual Discriminator GANs），通过结合双判别器结构和可调损失函数（α-loss）来缓解生成对抗网络中的模式崩溃问题。作者进一步将这一方法推广到任意定义在正实数上的函数，从而形成了一类更广泛的模型。论文还提供了理论分析，表明相关的极小极大优化可以简化为f-散度和反向f-散度的线性组合。实验部分在2D合成数据上进行，并使用多种性能指标验证了所提模型的优势。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17684

### 59. HydraOpt: Navigating the Efficiency-Performance Trade-off of Adapter Merging
#### Abstract:
Large language models (LLMs) often leverage adapters, such as low-rank-based adapters, to achieve strong performance on downstream tasks. However, storing a separate adapter for each task significantly increases memory requirements, posing a challenge for resource-constrained environments such as mobile devices. Although model merging techniques can reduce storage costs, they typically result in substantial performance degradation. In this work, we introduce HydraOpt, a new model merging technique that capitalizes on the inherent similarities between the matrices of low-rank adapters. Unlike existing methods that produce a fixed trade-off between storage size and performance, HydraOpt allows us to navigate this spectrum of efficiency and performance. Our experiments show that HydraOpt significantly reduces storage size (48% reduction) compared to storing all adapters, while achieving competitive performance (0.2-1.8% drop). Furthermore, it outperforms existing merging techniques in terms of performance at the same or slightly worse storage efficiency.
#### Summary:
这篇论文介绍了HydraOpt，一种新的模型合并技术，旨在解决大型语言模型（LLMs）中适配器存储需求高的问题。通过利用低秩适配器矩阵之间的相似性，HydraOpt在存储效率和性能之间提供了可调节的权衡。实验表明，HydraOpt显著减少了存储空间（减少48%），同时保持了竞争力的性能（性能下降0.2-1.8%）。
#### Relevance Score: 6.0
#### PDF URL: https://arxiv.org/pdf/2507.17706

### 60. HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting
#### Abstract:
The exponential growth of video content has made personalized video highlighting an essential task, as user preferences are highly variable and complex. Existing video datasets, however, often lack personalization, relying on isolated videos or simple text queries that fail to capture the intricacies of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for personalized video highlighting, created using an LLM-based user simulator to generate realistic watch histories reflecting diverse user preferences. The dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400 videos across 170 semantic categories. To validate our dataset, we propose HiPHer, a method that leverages these personalized watch histories to predict preference-conditioned segment-wise saliency scores. Through extensive experiments, we demonstrate that our method outperforms existing generic and query-based approaches, showcasing its potential for highly user-centric video highlighting in real-world scenarios.
#### Summary:
这篇论文介绍了HIPPO-Video，一个用于个性化视频高亮显示的新数据集，该数据集通过基于LLM的用户模拟器生成反映多样化用户偏好的观看历史。数据集包含2,040对（观看历史，显著性分数）数据，覆盖20,400个视频和170个语义类别。论文还提出了HiPHer方法，利用这些个性化观看历史来预测偏好条件下的片段显著性分数，并通过实验证明其优于现有的通用和基于查询的方法。
#### Relevance Score: 5.0
#### PDF URL: https://arxiv.org/pdf/2507.16873

